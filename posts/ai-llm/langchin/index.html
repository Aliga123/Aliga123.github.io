<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Langchin | aliga&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="ç¯å¢ƒè®¾ç½® # åŠ è½½ç¯å¢ƒå˜é‡ from dotenv import load_dotenv, find_dotenv import os _ = load_dotenv(find_dotenv()) # è¯»å–æœ¬åœ° .env æ–‡ä»¶ï¼Œé‡Œé¢å®šä¹‰äº† OPENAI_API_KEY ï¼ˆå¯é€‰ï¼‰å¯ç”¨LangSmithï¼šæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¯æ¬¡è¿è¡Œè¢«è®°å½•åˆ° LangSmithï¼ˆå…è´¹æ˜¯4000æ¬¡ï¼‰ï¼Œå¹¶ä¸”å¯ä»¥çœ‹åˆ°LangSmith çš„è·Ÿè¸ªhttps://smith.langchain.com/public/8">
<meta name="author" content="aliga">
<link rel="canonical" href="https://Aliga123.github.io/posts/ai-llm/langchin/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://Aliga123.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="16x16" href="https://Aliga123.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="32x32" href="https://Aliga123.github.io/img/Q.gif">
<link rel="apple-touch-icon" href="https://Aliga123.github.io/img/Q.gif">
<link rel="mask-icon" href="https://Aliga123.github.io/img/Q.gif">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://Aliga123.github.io/posts/ai-llm/langchin/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script defer src="https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js"></script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = ""; 
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

  

<meta property="og:title" content="Langchin" />
<meta property="og:description" content="ç¯å¢ƒè®¾ç½® # åŠ è½½ç¯å¢ƒå˜é‡ from dotenv import load_dotenv, find_dotenv import os _ = load_dotenv(find_dotenv()) # è¯»å–æœ¬åœ° .env æ–‡ä»¶ï¼Œé‡Œé¢å®šä¹‰äº† OPENAI_API_KEY ï¼ˆå¯é€‰ï¼‰å¯ç”¨LangSmithï¼šæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¯æ¬¡è¿è¡Œè¢«è®°å½•åˆ° LangSmithï¼ˆå…è´¹æ˜¯4000æ¬¡ï¼‰ï¼Œå¹¶ä¸”å¯ä»¥çœ‹åˆ°LangSmith çš„è·Ÿè¸ªhttps://smith.langchain.com/public/8" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Aliga123.github.io/posts/ai-llm/langchin/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-07-28T00:18:23+08:00" />
<meta property="article:modified_time" content="2024-07-28T00:18:23+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Langchin"/>
<meta name="twitter:description" content="ç¯å¢ƒè®¾ç½® # åŠ è½½ç¯å¢ƒå˜é‡ from dotenv import load_dotenv, find_dotenv import os _ = load_dotenv(find_dotenv()) # è¯»å–æœ¬åœ° .env æ–‡ä»¶ï¼Œé‡Œé¢å®šä¹‰äº† OPENAI_API_KEY ï¼ˆå¯é€‰ï¼‰å¯ç”¨LangSmithï¼šæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¯æ¬¡è¿è¡Œè¢«è®°å½•åˆ° LangSmithï¼ˆå…è´¹æ˜¯4000æ¬¡ï¼‰ï¼Œå¹¶ä¸”å¯ä»¥çœ‹åˆ°LangSmith çš„è·Ÿè¸ªhttps://smith.langchain.com/public/8"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  1 ,
          "name": "ğŸ“šæ–‡ç« ",
          "item": "https://Aliga123.github.io/posts/"
        },

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "ğŸ§± AIå¤§æ¨¡å‹åº”ç”¨å¼€å‘",
          "item": "https://Aliga123.github.io/posts/ai-llm/"
        }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Langchin",
      "item": "https://Aliga123.github.io/posts/ai-llm/langchin/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Langchin",
  "name": "Langchin",
  "description": "ç¯å¢ƒè®¾ç½® # åŠ è½½ç¯å¢ƒå˜é‡ from dotenv import load_dotenv, find_dotenv import os _ = load_dotenv(find_dotenv()) # è¯»å–æœ¬åœ° .env æ–‡ä»¶ï¼Œé‡Œé¢å®šä¹‰äº† OPENAI_API_KEY ï¼ˆå¯é€‰ï¼‰å¯ç”¨LangSmithï¼šæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¯æ¬¡è¿è¡Œè¢«è®°å½•åˆ° LangSmithï¼ˆå…è´¹æ˜¯4000æ¬¡ï¼‰ï¼Œå¹¶ä¸”å¯ä»¥çœ‹åˆ°LangSmith çš„è·Ÿè¸ªhttps://smith.langchain.com/public/8",
  "keywords": [
    ""
  ],
  "articleBody": "ç¯å¢ƒè®¾ç½®\n# åŠ è½½ç¯å¢ƒå˜é‡ from dotenv import load_dotenv, find_dotenv import os _ = load_dotenv(find_dotenv()) # è¯»å–æœ¬åœ° .env æ–‡ä»¶ï¼Œé‡Œé¢å®šä¹‰äº† OPENAI_API_KEY ï¼ˆå¯é€‰ï¼‰å¯ç”¨LangSmithï¼šæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¯æ¬¡è¿è¡Œè¢«è®°å½•åˆ° LangSmithï¼ˆå…è´¹æ˜¯4000æ¬¡ï¼‰ï¼Œå¹¶ä¸”å¯ä»¥çœ‹åˆ°LangSmith çš„è·Ÿè¸ªhttps://smith.langchain.com/public/88baa0b2-7c1a-4d09-ba30-a47985dde2ea/r\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") LangChain çš„æ ¸å¿ƒåŠŸèƒ½ åŠ è½½å¤§æ¨¡å‹ 1.API\nåœ¨æ„å»ºChatModelæ—¶ï¼Œæˆ‘ä»¬æœ‰ä¸€äº›æ ‡å‡†åŒ–çš„å‚æ•°:\nmodelï¼šæ¨¡å‹çš„åç§° temperatureï¼šé‡‡æ ·æ¸©åº¦ timeoutï¼š è¯·æ±‚è¶…æ—¶ max_tokensï¼šç”Ÿæˆçš„æœ€å¤§ä»¤ç‰Œæ•° stopï¼šé»˜è®¤åœæ­¢åºåˆ— max_retriesï¼šé‡è¯•è¯·æ±‚çš„æœ€å¤§æ¬¡æ•° api_keyï¼šæ¨¡å‹æä¾›è€…çš„ API å¯†é’¥ base_urlï¼šå‘é€è¯·æ±‚çš„ç«¯ç‚¹ å¤§æ¨¡å‹å‚è€ƒAPIï¼šhttps://python.langchain.com/v0.2/docs/integrations/platforms/\n# 1.openai os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") from langchain_openai import ChatOpenAI model = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\") # 2.Azure os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\") from langchain_openai import AzureChatOpenAI llm = AzureChatOpenAI( azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"], azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"], openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"], ) 2.æœ¬åœ°æ¨¡å‹\nï¼ˆ1ï¼‰è¯„ä¼°\nè¿™äº›æ³•å­¦ç¡•å£«å¯ä»¥ä»è‡³å°‘ä¸¤ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ï¼š\nBase modelï¼šåŸºç¡€æ¨¡å‹æ˜¯ä»€ä¹ˆä»¥åŠå®ƒæ˜¯å¦‚ä½•è®­ç»ƒçš„ï¼Ÿ Fine-tuning approachï¼šåŸºç¡€æ¨¡å‹æ˜¯å¦ç»è¿‡å¾®è°ƒï¼Ÿå¦‚æœæ˜¯ï¼Œä½¿ç”¨äº†å“ªç»„æŒ‡ä»¤ï¼Ÿ å¯ä»¥ä½¿ç”¨å¤šä¸ªæ’è¡Œæ¦œæ¥è¯„ä¼°è¿™äº›æ¨¡å‹çš„ç›¸å¯¹æ€§èƒ½ï¼Œå…¶ä¸­åŒ…æ‹¬ï¼š\nç³»ç»Ÿç®¡ç†è½¯ä»¶ GPT4All æ‹¥æŠ±è„¸ ï¼ˆ2ï¼‰æ”¯æŒåœ¨å„ç§è®¾å¤‡ä¸Šæ¨ç†å¼€æº LLM çš„æ¡†æ¶\nllama.cppï¼šå¸¦æœ‰æƒé‡ä¼˜åŒ–/é‡åŒ–çš„ llama æ¨ç†ä»£ç çš„ C++ å®ç° gpt4allï¼šä¼˜åŒ– C åç«¯ä»¥è¿›è¡Œæ¨ç† Ollamaï¼šå°†æ¨¡å‹æƒé‡å’Œç¯å¢ƒæ†ç»‘åˆ°åœ¨è®¾å¤‡ä¸Šè¿è¡Œå¹¶ä¸º LLM æä¾›æœåŠ¡çš„åº”ç”¨ç¨‹åºä¸­ llamafileï¼šå°†æ¨¡å‹æƒé‡å’Œè¿è¡Œæ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰å†…å®¹æ†ç»‘åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œä½¿æ‚¨å¯ä»¥ä»æ­¤æ–‡ä»¶åœ¨æœ¬åœ°è¿è¡Œ LLMï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„å®‰è£…æ­¥éª¤ ä¸€èˆ¬æ¥è¯´ï¼Œè¿™äº›æ¡†æ¶ä¼šåšå‡ ä»¶äº‹ï¼š\nQuantizationï¼šå‡å°‘åŸå§‹æ¨¡å‹æƒé‡çš„å†…å­˜å ç”¨ Efficient implementation for inferenceï¼šæ”¯æŒåœ¨æ¶ˆè´¹ç¡¬ä»¶ï¼ˆä¾‹å¦‚ CPU æˆ–ç¬”è®°æœ¬ç”µè„‘ GPUï¼‰ä¸Šè¿›è¡Œæ¨ç† Prompt 1.PromptTemplateï¼šé€‚ç”¨äºåªéœ€è¦ç”¨æˆ·è¾“å…¥\nfrom langchain_core.prompts import PromptTemplate prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\") prompt_template.invoke({\"topic\": \"cats\"}) # éƒ¨åˆ†æ ¼å¼åŒ–æç¤ºæ¨¡æ¿ prompt = PromptTemplate( template=\"Tell me a {adjective} joke about the day {date}\", input_variables=[\"adjective\"], partial_variables={\"date\": \"2024.7.23\"}, ) print(prompt.format(adjective=\"funny\")) 2.ChatPromptTemplatesï¼šåŒ…å«systemã€user\nfrom langchain_core.prompts import ChatPromptTemplate prompt_template = ChatPromptTemplate.from_messages([ (\"system\", \"You are a helpful assistant\"), (\"user\", \"Tell me a joke about {topic}\") ]) prompt_template.invoke({\"topic\": \"cats\"}) 3.MessagesPlaceholderï¼šæ¶ˆæ¯å ä½ç¬¦\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain_core.messages import HumanMessage prompt_template = ChatPromptTemplate.from_messages([ (\"system\", \"You are a helpful assistant\"), MessagesPlaceholder(\"msgs\") ]) prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]}) # æˆ–è€…ç¬¬äºŒç§è¡¨ç¤º prompt_template = ChatPromptTemplate.from_messages([ (\"system\", \"You are a helpful assistant\"), (\"placeholder\", \"{msgs}\") # \u003c-- This is the changed part ]) prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]}) 4.FewShotPromptTemplateã€FewShotChatMessagePromptTemplateï¼šç»™promptæ·»åŠ å°‘é‡æ ·æœ¬\nexamples = [ { \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\", \"answer\": \"\"\" Are follow up questions needed here: Yes. Follow up: How old was Muhammad Ali when he died? Intermediate answer: Muhammad Ali was 74 years old when he died. Follow up: How old was Alan Turing when he died? Intermediate answer: Alan Turing was 41 years old when he died. So the final answer is: Muhammad Ali \"\"\", }, { \"question\": \"When was the founder of craigslist born?\", \"answer\": \"\"\" Are follow up questions needed here: Yes. Follow up: Who was the founder of craigslist? Intermediate answer: Craigslist was founded by Craig Newmark. Follow up: When was Craig Newmark born? Intermediate answer: Craig Newmark was born on December 6, 1952. So the final answer is: December 6, 1952 \"\"\", }, ] ï¼ˆ1ï¼‰PromptTemplate + FewShotPromptTemplate\nfrom langchain_core.prompts import PromptTemplate example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\") from langchain_core.prompts import FewShotPromptTemplate prompt = FewShotPromptTemplate( examples=examples, example_prompt=example_prompt, suffix=\"Question: {input}\", input_variables=[\"input\"], ) print( prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string() ) from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate # This is a prompt template used to format each individual example. example_prompt = ChatPromptTemplate.from_messages( [ (\"human\", \"Question: {question}\"), (\"ai\", \"{answer}\"), ] ) few_shot_prompt = FewShotChatMessagePromptTemplate( example_prompt=example_prompt, examples=examples, ) #print(few_shot_prompt.invoke({}).to_messages()) final_prompt = ChatPromptTemplate.from_messages( [ (\"system\", \"You are a QA bot.\"), few_shot_prompt, (\"human\", \"{input}\"), ] ) final_prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}) 5.messages:ç”±ä¸€ç³»åˆ—æ¶ˆæ¯ç»„æˆ\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage # æ³¨æ„ï¼šæœ€åç”¨æˆ·çš„è¾“å…¥å¿…é¡»æ˜¯\"{}\"çš„å½¢å¼ï¼Œä¸èƒ½ä½¿ç”¨HumanMessage prompt = ( SystemMessage(content=\"You are a nice Mathematician.\") + HumanMessage(content=\"Can you help me with some math?\") + AIMessage(content=\"Sure, tell me your question.\") + \"{user_input}\" ) query = \"What is 3 * 12? Also, what is 11 + 49?\" #prompt.invoke({\"user_input\": query}) prompt.format_messages(user_input=query) prompt.append(AIMessage(content=\"Sure, tell me your question.\")) prompt.format_messages(user_input=query) 6.PipelinePromptTemplate:æƒ³è¦é‡ç”¨æç¤ºçš„éƒ¨åˆ†å†…å®¹æ—¶ï¼Œè¯¥ç±»éå¸¸æœ‰ç”¨\nfrom langchain_core.prompts import PipelinePromptTemplate, PromptTemplate full_prompt = PromptTemplate.from_template( \"\"\"{introduction} {example} {start}\"\"\") introduction_prompt = PromptTemplate.from_template(\"You are impersonating {person}.\") example_prompt = PromptTemplate.from_template( \"\"\"Here's an example of an interaction: Q: {example_q} A: {example_a}\"\"\") start_prompt = PromptTemplate.from_template( \"\"\"Now, do this for real! Q: {input} A:\"\"\") input_prompts = [ (\"introduction\", introduction_prompt), (\"example\", example_prompt), (\"start\", start_prompt), ] pipeline_prompt = PipelinePromptTemplate( final_prompt=full_prompt, pipeline_prompts=input_prompts ) pipeline_prompt.input_variables print( pipeline_prompt.format( person=\"Elon Musk\", example_q=\"What's your favorite car?\", example_a=\"Tesla\", input=\"What's your favorite social media site?\", ) ) æ ¼å¼åŒ–è¾“å‡º æ–¹æ³•ä¸€ï¼šä½¿ç”¨pydanticï¼Œè¿”å›ä¸€ä¸ªpydanticå¯¹è±¡ã€‚å¯ä»¥æ ¹æ®è¾“å…¥å†…å®¹è‡ªåŠ¨é€‰æ‹©è¾“å‡ºæ ¼å¼\nfrom typing import Optional from typing import Union from langchain_core.pydantic_v1 import BaseModel, Field class Joke(BaseModel): \"\"\"Joke to tell user.\"\"\" setup: str = Field(description=\"The setup of the joke\") punchline: str = Field(description=\"The punchline to the joke\") rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\") class ConversationalResponse(BaseModel): \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\" response: str = Field(description=\"A conversational response to the user's query\") # è®©æ¨¡å‹ä»å¤šä¸ªæ¨¡å¼ä¸­è¿›è¡Œè‡ªåŠ¨é€‰æ‹© class Response(BaseModel): output: Union[Joke, ConversationalResponse] structured_llm = model.with_structured_output(Response) structured_llm.invoke(\"How are you today?\") #structured_llm.invoke(\"Tell me a joke about cats\") æ–¹æ³•äºŒï¼šJSON Schemaï¼Œè¿”å›ä¸€ä¸ªå­—å…¸\njson_schema = { \"title\": \"joke\", \"description\": \"Joke to tell user.\", \"type\": \"object\", \"properties\": { \"setup\": { \"type\": \"string\", \"description\": \"The setup of the joke\", }, \"punchline\": { \"type\": \"string\", \"description\": \"The punchline to the joke\", }, \"rating\": { \"type\": \"integer\", \"description\": \"How funny the joke is, from 1 to 10\", }, }, \"required\": [\"setup\", \"punchline\"], } structured_llm = model.with_structured_output(json_schema) structured_llm.invoke(\"Tell me a joke about cats\") æ–¹æ³•ä¸‰ï¼šå°‘é‡ç¤ºä¾‹ï¼ˆfew-shotï¼‰\nfrom langchain_core.prompts import ChatPromptTemplate system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\ Return a joke which has the setup (the response to \"Who's there?\") and the final punchline (the response to \" who?\"). Here are some examples of jokes: example_user: Tell me a joke about planes example_assistant: {{\"setup\": \"Why don't planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}} example_user: Tell me another joke about planes example_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\", \"rating\": 10}} example_user: Now about caterpillars example_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\" prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")]) few_shot_structured_llm = prompt | model few_shot_structured_llm.invoke(\"what's something funny about woodpeckers\") **æ–¹æ³•å››ï¼š**å¹¶éæ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒ.with_structured_output()ï¼Œå› æ­¤ä½¿ç”¨å†…ç½®å‡½æ•°PydanticOutputParseræ¥è§£ææç¤ºä¸ç»™å®šçš„ Pydantic æ¨¡å¼åŒ¹é…çš„llmçš„è¾“å‡ºåˆ°Promptä¸­ã€‚\nfrom typing import List from langchain_core.output_parsers import PydanticOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_core.pydantic_v1 import BaseModel, Field class Person(BaseModel): \"\"\"Information about a person.\"\"\" name: str = Field(..., description=\"The name of the person\") height_in_meters: float = Field( ..., description=\"The height of the person expressed in meters.\" ) class People(BaseModel): \"\"\"Identifying information about all people in a text.\"\"\" people: List[Person] # Set up a parser parser = PydanticOutputParser(pydantic_object=People) # Prompt prompt = ChatPromptTemplate.from_messages( [ ( \"system\", \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\", ), (\"human\", \"{query}\"), ] ).partial(format_instructions=parser.get_format_instructions()) query = \"Anna is 23 years old and she is 6 feet tall\" print(prompt.invoke(query).to_string()) # å†æ¬¡ä½¿ç”¨parserï¼Œåˆå¯ä»¥æŠŠè¾“å‡ºè½¬æ¢æˆParserå¯¹è±¡ chain = prompt | model | parser chain.invoke({\"query\": query}) # Prompt prompt = ChatPromptTemplate.from_messages( [ ( \"system\", \"Answer the user query. Wrap the output in `json` tags\\n{schema}\", ), (\"human\", \"{query}\"), ] ).partial(schema=People.schema()) query = \"Anna is 23 years old and she is 6 feet tall\" print(prompt.format_prompt(query=query).to_string()) æ„å»ºchain 1. LangChain Expression Language (LCEL): |\nfrom langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate prompt = ChatPromptTemplate.from_template(\"tell me a joke about {user_input}\") chain = prompt | model | StrOutputParser() chain.invoke({\"user_input\": \"bears\"}) # ç¬¬äºŒç§æ–¹æ³• composed_chain = {\"user_input\": chain} | prompt | model | StrOutputParser() composed_chain.invoke({\"user_input\": \"bears\"}) #composed_chain.invoke({\"bears\"}) #åªæœ‰ä¸€ä¸ªå‚æ•°è¿˜å¯ä»¥ç›´æ¥ä¼  # ç¬¬ä¸‰ç§æ–¹æ³•ï¼ŒRunnablePassthroughç›´æ¥è¿‡å»åŸå‚æ•° from langchain_core.runnables import RunnablePassthrough Runnable_chain = {\"user_input\": RunnablePassthrough()} | prompt | model | StrOutputParser() Runnable_chain.invoke(\"bears\") 2. pipe()\nfrom langchain_core.runnables import RunnableParallel composed_chain_with_pipe = RunnableParallel({\"user_input\": chain}).pipe( prompt, model, StrOutputParser() ) composed_chain_with_pipe.invoke({\"user_input\": \"bears\"}) æ·»åŠ å†å²è®°å½• 1.ä½¿ç”¨SQLiteæ•°æ®åº“ä¿å­˜\nfrom langchain_community.chat_message_histories import SQLChatMessageHistory # session_idæ˜¯è¿™äº›è¾“å…¥æ¶ˆæ¯å¯¹åº”çš„ä¼šè¯ï¼ˆå¯¹è¯ï¼‰çº¿ç¨‹çš„æ ‡è¯†ç¬¦ã€‚è¿™å…è®¸æ‚¨åŒæ—¶ç»´æŠ¤åŒä¸€é“¾ä¸­çš„å¤šä¸ªå¯¹è¯/çº¿ç¨‹ã€‚ def get_session_history(session_id): return SQLChatMessageHistory(session_id, \"sqlite:///memory.db\") # å†å²å¯¹è¯ä¼šä¿å­˜åœ¨å½“å‰é¡¹ç›®çš„memory.dbæ–‡ä»¶é‡Œ from langchain_core.messages import HumanMessage from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain_core.runnables.history import RunnableWithMessageHistory prompt = ChatPromptTemplate.from_messages( [ ( \"system\", \"You're an assistant who speaks in {language}. Respond in 20 words or fewer\", ), MessagesPlaceholder(variable_name=\"history\"), (\"human\", \"{input}\"), ] ) runnable = prompt | model runnable_with_history = RunnableWithMessageHistory( runnable, get_session_history, input_messages_key=\"input\", history_messages_key=\"history\", ) runnable_with_history.invoke( {\"language\": \"italian\", \"input\": \"hi im bob!\"}, config={\"configurable\": {\"session_id\": \"2\"}}, ) # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸Šä¸‹æ–‡é€šè¿‡æä¾›çš„èŠå¤©å†å²è®°å½•è¿›è¡Œä¿ç•™session_idï¼Œå› æ­¤æ¨¡å‹çŸ¥é“ç”¨æˆ·çš„å§“åã€‚ runnable_with_history.invoke( {\"language\": \"italian\", \"input\": \"whats my name?\"}, config={\"configurable\": {\"session_id\": \"2\"}}, ) 2.ç”¨æˆ·å®šåˆ¶\nfrom langchain_core.runnables import ConfigurableFieldSpec def get_session_history(user_id: str, conversation_id: str): return SQLChatMessageHistory(f\"{user_id}--{conversation_id}\", \"sqlite:///memory.db\") with_message_history = RunnableWithMessageHistory( runnable, get_session_history, input_messages_key=\"input\", history_messages_key=\"history\", history_factory_config=[ ConfigurableFieldSpec( id=\"user_id\", annotation=str, name=\"User ID\", description=\"Unique identifier for the user.\", default=\"\", is_shared=True, ), ConfigurableFieldSpec( id=\"conversation_id\", annotation=str, name=\"Conversation ID\", description=\"Unique identifier for the conversation.\", default=\"\", is_shared=True, ), ], ) with_message_history.invoke( {\"language\": \"italian\", \"input\": \"hi im bob!\"}, config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}}, ) Toolsï¼ˆfunction callingï¼‰ 1.åˆ›å»ºtoolå’Œè°ƒç”¨\næ–¹å¼ä¸€ï¼š@toolè£…é¥°å™¨+StructuredTool\nfrom langchain_core.tools import tool # parse_docstring=Trueå¯¹æ–‡æ¡£å­—ç¬¦ä¸²è¿›è¡Œè§£æã€‚æ³¨æ„ï¼šå‡½æ•°æè¿°å’Œå‚æ•°æè¿°ä¸­é—´è¦ç©ºä¸€è¡Œã€‚ @tool(\"multiplication-tool\", parse_docstring=True, return_direct=True) def multiply(a: int, b: int) -\u003e int: \"\"\"Multiply two numbers. Args: a: first number. b: second number. \"\"\" return a * b print(multiply.invoke({\"a\": 2, \"b\": 3})) # Let's inspect some of the attributes associated with the tool. print(multiply.name) print(multiply.description) print(multiply.args) print(multiply.return_direct) multiply.args_schema.schema() æ–¹å¼äºŒï¼š@toolè£…é¥°å™¨+pydantic\nfrom langchain_core.tools import tool from langchain.pydantic_v1 import BaseModel, Field # ç»™å‚æ•°æ·»åŠ æè¿° class CalculatorInput(BaseModel): a: int = Field(description=\"first number\") b: int = Field(description=\"second number\") @tool(\"multiplication-tool\", args_schema=CalculatorInput, return_direct=True) def multiply(a: int, b: int) -\u003e int: \"\"\"Multiply two numbers.\"\"\" return a * b print(multiply.invoke({\"a\": 2, \"b\": 3})) # Let's inspect some of the attributes associated with the tool. print(multiply.name) print(multiply.description) print(multiply.args) print(multiply.return_direct) multiply.args_schema.schema() æ–¹å¼ä¸‰ï¼šStructuredTool+pydantic\nfrom langchain_core.tools import StructuredTool from langchain.pydantic_v1 import BaseModel, Field # å¯¹å‚æ•°çš„æè¿° class CalculatorInput(BaseModel): a: int = Field(description=\"first number\") b: int = Field(description=\"second number\") def multiply(a: int, b: int) -\u003e int: \"\"\"Multiply two numbers.\"\"\" return a * b async def amultiply(a: int, b: int) -\u003e int: \"\"\"Multiply two numbers.\"\"\" return a * b calculator = StructuredTool.from_function( func=multiply, name=\"Calculator\", description=\"multiply numbers\", args_schema=CalculatorInput, return_direct=True, coroutine= amultiply #\u003c- å¦‚æœéœ€è¦ï¼Œæ‚¨ä¹Ÿå¯ä»¥æŒ‡å®šå¼‚æ­¥æ–¹æ³• ) print(calculator.invoke({\"a\": 2, \"b\": 3})) print(await calculator.ainvoke({\"a\": 4, \"b\": 3})) print(calculator.name) print(calculator.description) print(calculator.args) æ–¹å¼å››ï¼šå°†chainä½œä¸ºå·¥å…·+.as_tool()\nfrom langchain_core.language_models import GenericFakeChatModel from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate prompt = ChatPromptTemplate.from_messages( [(\"human\", \"Hello. Please respond in the style of {answer_style}.\")] ) # Placeholder LLM llm = GenericFakeChatModel(messages=iter([\"hello matey\"])) chain = prompt | llm | StrOutputParser() as_tool = chain.as_tool( name=\"Style responder\", description=\"Description of when to use tool.\" ) as_tool.args æ–¹å¼äº”ï¼šå­ç±»åŒ–åˆ›å»ºå·¥å…·ï¼ŒBaseModel+BaseTool\nfrom typing import Optional, Type from langchain.pydantic_v1 import BaseModel from langchain_core.callbacks import ( AsyncCallbackManagerForToolRun, CallbackManagerForToolRun, ) from langchain_core.tools import BaseTool class CalculatorInput(BaseModel): a: int = Field(description=\"first number\") b: int = Field(description=\"second number\") class CustomCalculatorTool(BaseTool): name = \"Calculator\" description = \"useful for when you need to answer questions about math\" args_schema: Type[BaseModel] = CalculatorInput return_direct: bool = True def _run( self, a: int, b: int, run_manager: Optional[CallbackManagerForToolRun] = None ) -\u003e str: \"\"\"Use the tool.\"\"\" return a * b async def _arun( self, a: int, b: int, run_manager: Optional[AsyncCallbackManagerForToolRun] = None, ) -\u003e str: \"\"\"Use the tool asynchronously.\"\"\" # If the calculation is cheap, you can just delegate to the sync implementation # as shown below. # If the sync calculation is expensive, you should delete the entire _arun method. # LangChain will automatically provide a better implementation that will # kick off the task in a thread to make sure it doesn't block other async code. return self._run(a, b, run_manager=run_manager.get_sync()) multiply = CustomCalculatorTool() print(multiply.name) print(multiply.description) print(multiply.args) print(multiply.return_direct) print(multiply.invoke({\"a\": 2, \"b\": 3})) print(await multiply.ainvoke({\"a\": 2, \"b\": 3})) 2.åœ¨chatLLMæ‰§è¡Œtoolï¼ˆfunction callingï¼‰\n# ä½¿ç”¨è¯¥.bind_tools()æ–¹æ³•æ¥å¤„ç†è½¬æ¢ Multiplyä¸ºæ¨¡å‹çš„æ­£ç¡®æ ¼å¼ï¼Œç„¶åç»‘å®šå®ƒï¼ˆå³ï¼Œæ¯æ¬¡è°ƒç”¨æ¨¡å‹æ—¶ä¼ é€’å®ƒï¼‰ from langchain_core.tools import tool # ç®€å•çš„å®šä¹‰ä¸¤ä¸ªtool @tool def add(a: int, b: int) -\u003e int: \"\"\"Adds a and b.\"\"\" return a + b @tool def multiply(a: int, b: int) -\u003e int: \"\"\"Multiplies a and b.\"\"\" return a * b tools = [add, multiply] llm_with_tools = model.bind_tools(tools) from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage # å¯¹äºè¿™ç§promptçš„è®¾è®¡ï¼Œä¸é€‚åˆè¿ç»­å¯¹è¯ def llm_tool_calling(query): prompt = ( SystemMessage(content=\"You are a nice Mathematician.\") + HumanMessage(content=\"Can you help me with some math?\") + AIMessage(content=\"Sure, tell me your question.\") + \"{user_input}\" ) chain = prompt | llm_with_tools ai_msg = chain.invoke({\"user_input\": query}) prompt.append(ai_msg) #prompt.format_messages(user_input=query) if ai_msg.content == \"\": \"\"\"Simple sequential tool calling helper.\"\"\" tool_map = {tool.name: tool for tool in tools} tool_calls = ai_msg.tool_calls.copy() # è¿”å›ä¸€ä¸ªå­—å…¸åˆ—è¡¨ for tool_call in tool_calls: # å¯¹@toolå‡½æ•°æ‰§è¡Œ.invoke(tool_call)è¿”å›çš„æ˜¯ToolMessageå¯¹è±¡,æ‰§è¡Œtool_call[\"args\"]è¿”å›å‡½æ•°ç»“æœ tool_msg = tool_map[tool_call[\"name\"]].invoke(tool_call) prompt.append(tool_msg) result = chain.invoke({\"user_input\": query}).content else: result = ai_msg.content return result # ç»‘å®štoolsçš„LLMä¼šè‡ªè¡Œåˆ¤æ–­æ˜¯å¦è°ƒç”¨toolï¼Œä»¥åŠè°ƒç”¨å“ªå‡ ä¸ªå‡ ä¸ªtoolã€‚æ‰€ä»¥åœ¨è¿™ä¹‹åå¯ä»¥åŠ ä¸€ä¸ªåˆ¤æ–­æ—¶å€™è¦æ‰§è¡Œtool query = \"What is 3 * 12?\" query = \"GPTæ˜¯ä»€ä¹ˆ?\" query = \"What is 3 * 12? Also, what is 11 + 49?\" print(llm_tool_calling(query)) 3.ä½¿ç”¨å†…ç½®å·¥å…·å’Œå·¥å…·åŒ…\nå„å·¥å…·åŒ…çš„ä½¿ç”¨æ–‡æ¡£ï¼šhttps://python.langchain.com/v0.2/docs/integrations/tools/\nä¸‹é¢ä»¥è°·æ­Œæœç´¢ä¸ºä¾‹ï¼š é¦–å…ˆï¼Œæ‚¨éœ€è¦è®¾ç½®æ­£ç¡®çš„ API å¯†é’¥å’Œç¯å¢ƒå˜é‡ã€‚è¦è¿›è¡Œè®¾ç½®ï¼Œè¯·åœ¨ Google Cloud å‡­æ®æ§åˆ¶å° ( https://console.cloud.google.com/apis/credentials ) ä¸­åˆ›å»º GOOGLE_API_KEYï¼Œå¹¶ä½¿ç”¨å¯ç¼–ç¨‹æœç´¢å¼•æ“ ( https://programmablesearchengine.google.com/controlpanel/create ) åˆ›å»º GOOGLE_CSE_IDã€‚æ¥ä¸‹æ¥ï¼Œæœ€å¥½æŒ‰ç…§æ­¤å¤„ï¼ˆhttps://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-searchï¼‰çš„è¯´æ˜è¿›è¡Œæ“ä½œã€‚\n!pip install google-api-python-client # æ£€æŸ¥æ˜¯å¦å¯ä»¥è°ƒç”¨Google API from googleapiclient.discovery import build my_api_key = os.getenv(\"GOOGLE_API_KEY\") my_cse_id = os.getenv(\"GOOGLE_API_KEY\") def google_search(search_term, api_key, cse_id, **kwargs): service = build(\"customsearch\", \"v1\", developerKey=api_key) res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute() return res['items'] results = google_search('\"god is a woman\" \"thank you next\" \"7 rings\"', my_api_key, my_cse_id, num=10) for result in results: print(result) !pip install -U langchain-google-community import os os.environ[\"GOOGLE_CSE_ID\"] = os.getenv(\"GOOGLE_CSE_ID\") os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") from langchain_google_community import GoogleSearchAPIWrapper from langchain_core.tools import Tool search = GoogleSearchAPIWrapper() tool = Tool( name=\"google_search\", description=\"Search Google for recent results.\", func=search.run, ) tool.run(\"Obama's first name?\") RAG 1.SemanticSimilarityExampleSelector:ä½¿ç”¨åµŒå…¥æ¨¡å‹æ¥è®¡ç®—è¾“å…¥ä¸å°‘é‡æ ·æœ¬ç¤ºä¾‹ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶ä½¿ç”¨å‘é‡å­˜å‚¨æ¥æ‰§è¡Œæœ€è¿‘é‚»æœç´¢.\nfrom langchain_chroma import Chroma from langchain_core.example_selectors import SemanticSimilarityExampleSelector from langchain_openai import OpenAIEmbeddings example_selector = SemanticSimilarityExampleSelector.from_examples( # This is the list of examples available to select from. examples, # This is the embedding class used to produce embeddings which are used to measure semantic similarity. OpenAIEmbeddings(), # This is the VectorStore class that is used to store the embeddings and do a similarity search over. Chroma, # This is the number of examples to produce. k=1, ) # Select the most similar example to the input. question = \"Who was the father of Mary Ball Washington?\" selected_examples = example_selector.select_examples({\"question\": question}) print(f\"Examples most similar to the input: {question}\") for example in selected_examples: print(\"\\n\") for k, v in example.items(): print(f\"{k}: {v}\") prompt = FewShotPromptTemplate( example_selector=example_selector, example_prompt=example_prompt, suffix=\"Question: {input}\", input_variables=[\"input\"], ) print( prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string() ) from langchain_chroma import Chroma from langchain_core.example_selectors import SemanticSimilarityExampleSelector from langchain_openai import OpenAIEmbeddings examples = [ {\"input\": \"2 ğŸ¦œ 2\", \"output\": \"4\"}, {\"input\": \"2 ğŸ¦œ 3\", \"output\": \"5\"}, {\"input\": \"2 ğŸ¦œ 4\", \"output\": \"6\"}, {\"input\": \"What did the cow say to the moon?\", \"output\": \"nothing at all\"}, { \"input\": \"Write me a poem about the moon\", \"output\": \"One for the moon, and one for me, who are we to talk about the moon?\", }, ] to_vectorize = [\" \".join(example.values()) for example in examples] embeddings = OpenAIEmbeddings() vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples) example_selector = SemanticSimilarityExampleSelector( vectorstore=vectorstore, k=2, ) # The prompt template will load examples by passing the input do the `select_examples` method example_selector.select_examples({\"input\": \"horse\"}) æœªå®Œå¾…ç»­ï¼ï¼ï¼\n2.LlamaIndex\næœªå®Œå¾…ç»­ï¼ï¼ï¼\nå…¶ä»–åŠŸèƒ½ æœªå®Œå¾…ç»­ï¼ï¼ï¼\n",
  "wordCount" : "4529",
  "inLanguage": "zh",
  "datePublished": "2024-07-28T00:18:23+08:00",
  "dateModified": "2024-07-28T00:18:23+08:00",
  "author":[{
    "@type": "Person",
    "name": "aliga"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Aliga123.github.io/posts/ai-llm/langchin/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "aliga's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Aliga123.github.io/img/Q.gif"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://Aliga123.github.io/" accesskey="h" title="Aliga&#39;s Blog (Alt + H)">
            <img src="https://Aliga123.github.io/img/Q.gif" alt="logo" aria-label="logo"
                 height="35">Aliga&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://Aliga123.github.io/search" title="ğŸ” æœç´¢ (Alt &#43; /)" accesskey=/>
                <span>ğŸ” æœç´¢</span>
                </a>
            </li>
            <li>
                <a href="https://Aliga123.github.io/" title="ğŸ  ä¸»é¡µ">
                <span>ğŸ  ä¸»é¡µ</span>
                </a>
            </li>
            <li>
                <a href="https://Aliga123.github.io/posts" title="ğŸ“š æ–‡ç« ">
                <span>ğŸ“š æ–‡ç« </span>
                </a>
            </li>
            <li>
                <a href="https://Aliga123.github.io/tags" title="ğŸ§© æ ‡ç­¾">
                <span>ğŸ§© æ ‡ç­¾</span>
                </a>
            </li>
            <li>
                <a href="https://Aliga123.github.io/archives/" title="â±ï¸ æ—¶é—´è½´">
                <span>â±ï¸ æ—¶é—´è½´</span>
                </a>
            </li>
            <li>
                <a href="https://Aliga123.github.io/about" title="ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº">
                <span>ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº</span>
                </a>
            </li>
            <li>
                <a href="https://Aliga123.github.io/links" title="ğŸ¤ å‹é“¾">
                <span>ğŸ¤ å‹é“¾</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://Aliga123.github.io/">ğŸ  ä¸»é¡µ</a>&nbsp;Â»&nbsp;<a href="https://Aliga123.github.io/posts/">ğŸ“šæ–‡ç« </a>&nbsp;Â»&nbsp;<a href="https://Aliga123.github.io/posts/ai-llm/">ğŸ§± AIå¤§æ¨¡å‹åº”ç”¨å¼€å‘</a></div>
            <h1 class="post-title">
                Langchin
            </h1>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2024-07-28
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>4529å­—
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>10åˆ†é’Ÿ
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>aliga
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://Aliga123.github.io/tags/langchin/" style="color: var(--secondary)!important;">Langchin</a>
            </span>
        </span>
    </span>
</span>
<span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://Aliga123.github.io/"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId:  null , 
                                region:  null , 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">ç›®å½•</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#langchain-%e7%9a%84%e6%a0%b8%e5%bf%83%e5%8a%9f%e8%83%bd" aria-label="LangChain çš„æ ¸å¿ƒåŠŸèƒ½">LangChain çš„æ ¸å¿ƒåŠŸèƒ½</a><ul>
                        
                <li>
                    <a href="#%e5%8a%a0%e8%bd%bd%e5%a4%a7%e6%a8%a1%e5%9e%8b" aria-label="åŠ è½½å¤§æ¨¡å‹">åŠ è½½å¤§æ¨¡å‹</a></li>
                <li>
                    <a href="#prompt" aria-label="Prompt">Prompt</a></li>
                <li>
                    <a href="#%e6%a0%bc%e5%bc%8f%e5%8c%96%e8%be%93%e5%87%ba" aria-label="æ ¼å¼åŒ–è¾“å‡º">æ ¼å¼åŒ–è¾“å‡º</a></li>
                <li>
                    <a href="#%e6%9e%84%e5%bb%bachain" aria-label="æ„å»ºchain">æ„å»ºchain</a></li>
                <li>
                    <a href="#%e6%b7%bb%e5%8a%a0%e5%8e%86%e5%8f%b2%e8%ae%b0%e5%bd%95" aria-label="æ·»åŠ å†å²è®°å½•">æ·»åŠ å†å²è®°å½•</a></li>
                <li>
                    <a href="#toolsfunction-calling" aria-label="Toolsï¼ˆfunction callingï¼‰">Toolsï¼ˆfunction callingï¼‰</a></li>
                <li>
                    <a href="#rag" aria-label="RAG">RAG</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%85%b6%e4%bb%96%e5%8a%9f%e8%83%bd" aria-label="å…¶ä»–åŠŸèƒ½">å…¶ä»–åŠŸèƒ½</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        if (elements) {
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                    (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                    return element;
                }
            }) || activeElement

            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                if (element === activeElement){
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
                } else {
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
                }
            })
        }
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><p><strong>ç¯å¢ƒè®¾ç½®</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># åŠ è½½ç¯å¢ƒå˜é‡</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> dotenv <span style="color:#f92672">import</span> load_dotenv, find_dotenv
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>_ <span style="color:#f92672">=</span> load_dotenv(find_dotenv())  <span style="color:#75715e"># è¯»å–æœ¬åœ° .env æ–‡ä»¶ï¼Œé‡Œé¢å®šä¹‰äº† OPENAI_API_KEY</span>
</span></span></code></pre></div><p><strong>ï¼ˆå¯é€‰ï¼‰å¯ç”¨LangSmith</strong>ï¼šæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¯æ¬¡è¿è¡Œè¢«è®°å½•åˆ° LangSmithï¼ˆå…è´¹æ˜¯4000æ¬¡ï¼‰ï¼Œå¹¶ä¸”å¯ä»¥çœ‹åˆ°LangSmith çš„è·Ÿè¸ªhttps://smith.langchain.com/public/88baa0b2-7c1a-4d09-ba30-a47985dde2ea/r</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#34;LANGCHAIN_TRACING_V2&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;true&#34;</span>
</span></span><span style="display:flex;"><span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#34;LANGCHAIN_API_KEY&#34;</span>] <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;LANGCHAIN_API_KEY&#34;</span>)
</span></span></code></pre></div><h1 id="langchain-çš„æ ¸å¿ƒåŠŸèƒ½">LangChain çš„æ ¸å¿ƒåŠŸèƒ½<a hidden class="anchor" aria-hidden="true" href="#langchain-çš„æ ¸å¿ƒåŠŸèƒ½">#</a></h1>
<h2 id="åŠ è½½å¤§æ¨¡å‹">åŠ è½½å¤§æ¨¡å‹<a hidden class="anchor" aria-hidden="true" href="#åŠ è½½å¤§æ¨¡å‹">#</a></h2>
<p><strong>1.API</strong></p>
<p>åœ¨æ„å»ºChatModelæ—¶ï¼Œæˆ‘ä»¬æœ‰ä¸€äº›æ ‡å‡†åŒ–çš„å‚æ•°:</p>
<ul>
<li>modelï¼šæ¨¡å‹çš„åç§°</li>
<li>temperatureï¼šé‡‡æ ·æ¸©åº¦</li>
<li>timeoutï¼š è¯·æ±‚è¶…æ—¶</li>
<li>max_tokensï¼šç”Ÿæˆçš„æœ€å¤§ä»¤ç‰Œæ•°</li>
<li>stopï¼šé»˜è®¤åœæ­¢åºåˆ—</li>
<li>max_retriesï¼šé‡è¯•è¯·æ±‚çš„æœ€å¤§æ¬¡æ•°</li>
<li>api_keyï¼šæ¨¡å‹æä¾›è€…çš„ API å¯†é’¥</li>
<li>base_urlï¼šå‘é€è¯·æ±‚çš„ç«¯ç‚¹</li>
</ul>
<p>å¤§æ¨¡å‹å‚è€ƒAPIï¼šhttps://python.langchain.com/v0.2/docs/integrations/platforms/</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 1.openai</span>
</span></span><span style="display:flex;"><span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#34;OPENAI_API_KEY&#34;</span>] <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;OPENAI_API_KEY&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_openai <span style="color:#f92672">import</span> ChatOpenAI
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> ChatOpenAI(model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gpt-4o-mini-2024-07-18&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 2.Azure</span>
</span></span><span style="display:flex;"><span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#34;AZURE_OPENAI_API_KEY&#34;</span>] <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;AZURE_OPENAI_API_KEY&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_openai <span style="color:#f92672">import</span> AzureChatOpenAI
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>llm <span style="color:#f92672">=</span> AzureChatOpenAI(
</span></span><span style="display:flex;"><span>    azure_endpoint<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#34;AZURE_OPENAI_ENDPOINT&#34;</span>],
</span></span><span style="display:flex;"><span>    azure_deployment<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#34;AZURE_OPENAI_DEPLOYMENT_NAME&#34;</span>],
</span></span><span style="display:flex;"><span>    openai_api_version<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#34;AZURE_OPENAI_API_VERSION&#34;</span>],
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p><strong>2.<a href="https://python.langchain.com/v0.2/docs/how_to/local_llms/">æœ¬åœ°æ¨¡å‹</a></strong></p>
<p><strong>ï¼ˆ1ï¼‰è¯„ä¼°</strong></p>
<p>è¿™äº›æ³•å­¦ç¡•å£«å¯ä»¥ä»è‡³å°‘ä¸¤ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ï¼š</p>
<ol>
<li><code>Base model</code>ï¼šåŸºç¡€æ¨¡å‹æ˜¯ä»€ä¹ˆä»¥åŠå®ƒæ˜¯å¦‚ä½•è®­ç»ƒçš„ï¼Ÿ</li>
<li><code>Fine-tuning approach</code>ï¼šåŸºç¡€æ¨¡å‹æ˜¯å¦ç»è¿‡å¾®è°ƒï¼Ÿå¦‚æœæ˜¯ï¼Œä½¿ç”¨äº†å“ª<a href="https://cameronrwolfe.substack.com/p/beyond-llama-the-power-of-open-llms#%C2%A7alpaca-an-instruction-following-llama-model">ç»„æŒ‡ä»¤ï¼Ÿ</a></li>
</ol>
<p>å¯ä»¥ä½¿ç”¨å¤šä¸ªæ’è¡Œæ¦œæ¥è¯„ä¼°è¿™äº›æ¨¡å‹çš„ç›¸å¯¹æ€§èƒ½ï¼Œå…¶ä¸­åŒ…æ‹¬ï¼š</p>
<ol>
<li><a href="https://chat.lmsys.org/?arena">ç³»ç»Ÿç®¡ç†è½¯ä»¶</a></li>
<li><a href="https://gpt4all.io/index.html">GPT4All</a></li>
<li><a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">æ‹¥æŠ±è„¸</a></li>
</ol>
<p><strong>ï¼ˆ2ï¼‰æ”¯æŒåœ¨å„ç§è®¾å¤‡ä¸Šæ¨ç†å¼€æº LLM çš„æ¡†æ¶</strong></p>
<ol>
<li><a href="https://github.com/ggerganov/llama.cpp"><code>llama.cpp</code></a><a href="https://finbarr.ca/how-is-llama-cpp-possible/">ï¼šå¸¦æœ‰æƒé‡ä¼˜åŒ–/é‡åŒ–</a>çš„ llama æ¨ç†ä»£ç çš„ C++ å®ç°</li>
<li><a href="https://docs.gpt4all.io/index.html"><code>gpt4all</code></a>ï¼šä¼˜åŒ– C åç«¯ä»¥è¿›è¡Œæ¨ç†</li>
<li><a href="https://ollama.ai/"><code>Ollama</code></a>ï¼šå°†æ¨¡å‹æƒé‡å’Œç¯å¢ƒæ†ç»‘åˆ°åœ¨è®¾å¤‡ä¸Šè¿è¡Œå¹¶ä¸º LLM æä¾›æœåŠ¡çš„åº”ç”¨ç¨‹åºä¸­</li>
<li><a href="https://github.com/Mozilla-Ocho/llamafile"><code>llamafile</code></a>ï¼šå°†æ¨¡å‹æƒé‡å’Œè¿è¡Œæ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰å†…å®¹æ†ç»‘åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œä½¿æ‚¨å¯ä»¥ä»æ­¤æ–‡ä»¶åœ¨æœ¬åœ°è¿è¡Œ LLMï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„å®‰è£…æ­¥éª¤</li>
</ol>
<p>ä¸€èˆ¬æ¥è¯´ï¼Œè¿™äº›æ¡†æ¶ä¼šåšå‡ ä»¶äº‹ï¼š</p>
<ol>
<li><code>Quantization</code>ï¼šå‡å°‘åŸå§‹æ¨¡å‹æƒé‡çš„å†…å­˜å ç”¨</li>
<li><code>Efficient implementation for inference</code>ï¼šæ”¯æŒåœ¨æ¶ˆè´¹ç¡¬ä»¶ï¼ˆä¾‹å¦‚ CPU æˆ–ç¬”è®°æœ¬ç”µè„‘ GPUï¼‰ä¸Šè¿›è¡Œæ¨ç†</li>
</ol>
<h2 id="prompt">Prompt<a hidden class="anchor" aria-hidden="true" href="#prompt">#</a></h2>
<p><strong>1.PromptTemplateï¼šé€‚ç”¨äºåªéœ€è¦ç”¨æˆ·è¾“å…¥</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.prompts <span style="color:#f92672">import</span> PromptTemplate
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt_template <span style="color:#f92672">=</span> PromptTemplate<span style="color:#f92672">.</span>from_template(<span style="color:#e6db74">&#34;Tell me a joke about </span><span style="color:#e6db74">{topic}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt_template<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;topic&#34;</span>: <span style="color:#e6db74">&#34;cats&#34;</span>})
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># éƒ¨åˆ†æ ¼å¼åŒ–æç¤ºæ¨¡æ¿</span>
</span></span><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> PromptTemplate(
</span></span><span style="display:flex;"><span>    template<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Tell me a </span><span style="color:#e6db74">{adjective}</span><span style="color:#e6db74"> joke about the day </span><span style="color:#e6db74">{date}</span><span style="color:#e6db74">&#34;</span>,
</span></span><span style="display:flex;"><span>    input_variables<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;adjective&#34;</span>],
</span></span><span style="display:flex;"><span>    partial_variables<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;date&#34;</span>: <span style="color:#e6db74">&#34;2024.7.23&#34;</span>},
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>print(prompt<span style="color:#f92672">.</span>format(adjective<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;funny&#34;</span>))
</span></span></code></pre></div><p><strong>2.ChatPromptTemplatesï¼šåŒ…å«systemã€user</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.prompts <span style="color:#f92672">import</span> ChatPromptTemplate
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt_template <span style="color:#f92672">=</span> ChatPromptTemplate<span style="color:#f92672">.</span>from_messages([
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;system&#34;</span>, <span style="color:#e6db74">&#34;You are a helpful assistant&#34;</span>),
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;Tell me a joke about </span><span style="color:#e6db74">{topic}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt_template<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;topic&#34;</span>: <span style="color:#e6db74">&#34;cats&#34;</span>})
</span></span></code></pre></div><p><strong>3.MessagesPlaceholderï¼šæ¶ˆæ¯å ä½ç¬¦</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.prompts <span style="color:#f92672">import</span> ChatPromptTemplate, MessagesPlaceholder
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.messages <span style="color:#f92672">import</span> HumanMessage
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt_template <span style="color:#f92672">=</span> ChatPromptTemplate<span style="color:#f92672">.</span>from_messages([
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;system&#34;</span>, <span style="color:#e6db74">&#34;You are a helpful assistant&#34;</span>),
</span></span><span style="display:flex;"><span>    MessagesPlaceholder(<span style="color:#e6db74">&#34;msgs&#34;</span>)
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt_template<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;msgs&#34;</span>: [HumanMessage(content<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;hi!&#34;</span>)]})
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># æˆ–è€…ç¬¬äºŒç§è¡¨ç¤º</span>
</span></span><span style="display:flex;"><span>prompt_template <span style="color:#f92672">=</span> ChatPromptTemplate<span style="color:#f92672">.</span>from_messages([
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;system&#34;</span>, <span style="color:#e6db74">&#34;You are a helpful assistant&#34;</span>),
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;placeholder&#34;</span>, <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{msgs}</span><span style="color:#e6db74">&#34;</span>) <span style="color:#75715e"># &lt;-- This is the changed part</span>
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>prompt_template<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;msgs&#34;</span>: [HumanMessage(content<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;hi!&#34;</span>)]})
</span></span></code></pre></div><p><strong>4.FewShotPromptTemplateã€FewShotChatMessagePromptTemplateï¼šç»™promptæ·»åŠ å°‘é‡æ ·æœ¬</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>examples <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    {
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;question&#34;</span>: <span style="color:#e6db74">&#34;Who lived longer, Muhammad Ali or Alan Turing?&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;answer&#34;</span>: <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Are follow up questions needed here: Yes.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Follow up: How old was Muhammad Ali when he died?
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Intermediate answer: Muhammad Ali was 74 years old when he died.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Follow up: How old was Alan Turing when he died?
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Intermediate answer: Alan Turing was 41 years old when he died.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">So the final answer is: Muhammad Ali
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>    {
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;question&#34;</span>: <span style="color:#e6db74">&#34;When was the founder of craigslist born?&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;answer&#34;</span>: <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Are follow up questions needed here: Yes.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Follow up: Who was the founder of craigslist?
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Intermediate answer: Craigslist was founded by Craig Newmark.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Follow up: When was Craig Newmark born?
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Intermediate answer: Craig Newmark was born on December 6, 1952.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">So the final answer is: December 6, 1952
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>]
</span></span></code></pre></div><p>ï¼ˆ1ï¼‰PromptTemplate + FewShotPromptTemplate</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.prompts <span style="color:#f92672">import</span> PromptTemplate
</span></span><span style="display:flex;"><span>example_prompt <span style="color:#f92672">=</span> PromptTemplate<span style="color:#f92672">.</span>from_template(<span style="color:#e6db74">&#34;Question: </span><span style="color:#e6db74">{question}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{answer}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.prompts <span style="color:#f92672">import</span> FewShotPromptTemplate
</span></span><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> FewShotPromptTemplate(
</span></span><span style="display:flex;"><span>    examples<span style="color:#f92672">=</span>examples,
</span></span><span style="display:flex;"><span>    example_prompt<span style="color:#f92672">=</span>example_prompt,
</span></span><span style="display:flex;"><span>    suffix<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Question: </span><span style="color:#e6db74">{input}</span><span style="color:#e6db74">&#34;</span>,
</span></span><span style="display:flex;"><span>    input_variables<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;input&#34;</span>],
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(
</span></span><span style="display:flex;"><span>    prompt<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;input&#34;</span>: <span style="color:#e6db74">&#34;Who was the father of Mary Ball Washington?&#34;</span>})<span style="color:#f92672">.</span>to_string()
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.prompts <span style="color:#f92672">import</span> ChatPromptTemplate, FewShotChatMessagePromptTemplate
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This is a prompt template used to format each individual example.</span>
</span></span><span style="display:flex;"><span>example_prompt <span style="color:#f92672">=</span> ChatPromptTemplate<span style="color:#f92672">.</span>from_messages(
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;human&#34;</span>, <span style="color:#e6db74">&#34;Question: </span><span style="color:#e6db74">{question}</span><span style="color:#e6db74">&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;ai&#34;</span>, <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{answer}</span><span style="color:#e6db74">&#34;</span>),
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>few_shot_prompt <span style="color:#f92672">=</span> FewShotChatMessagePromptTemplate(
</span></span><span style="display:flex;"><span>    example_prompt<span style="color:#f92672">=</span>example_prompt,
</span></span><span style="display:flex;"><span>    examples<span style="color:#f92672">=</span>examples,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#print(few_shot_prompt.invoke({}).to_messages())</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>final_prompt <span style="color:#f92672">=</span> ChatPromptTemplate<span style="color:#f92672">.</span>from_messages(
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;system&#34;</span>, <span style="color:#e6db74">&#34;You are a QA bot.&#34;</span>),
</span></span><span style="display:flex;"><span>        few_shot_prompt,
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;human&#34;</span>, <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{input}</span><span style="color:#e6db74">&#34;</span>),
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>final_prompt<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;input&#34;</span>: <span style="color:#e6db74">&#34;Who was the father of Mary Ball Washington?&#34;</span>})
</span></span></code></pre></div><p><strong>5.messages:ç”±ä¸€ç³»åˆ—æ¶ˆæ¯ç»„æˆ</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.messages <span style="color:#f92672">import</span> AIMessage, HumanMessage, SystemMessage
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># æ³¨æ„ï¼šæœ€åç”¨æˆ·çš„è¾“å…¥å¿…é¡»æ˜¯&#34;{}&#34;çš„å½¢å¼ï¼Œä¸èƒ½ä½¿ç”¨HumanMessage</span>
</span></span><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>    SystemMessage(content<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;You are a nice Mathematician.&#34;</span>) <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>    HumanMessage(content<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Can you help me with some math?&#34;</span>) <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>    AIMessage(content<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Sure, tell me your question.&#34;</span>) <span style="color:#f92672">+</span> 
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{user_input}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>query <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;What is 3 * 12? Also, what is 11 + 49?&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#prompt.invoke({&#34;user_input&#34;: query})</span>
</span></span><span style="display:flex;"><span>prompt<span style="color:#f92672">.</span>format_messages(user_input<span style="color:#f92672">=</span>query)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>prompt<span style="color:#f92672">.</span>append(AIMessage(content<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Sure, tell me your question.&#34;</span>))
</span></span><span style="display:flex;"><span>prompt<span style="color:#f92672">.</span>format_messages(user_input<span style="color:#f92672">=</span>query)
</span></span></code></pre></div><p><strong>6.PipelinePromptTemplate</strong>:æƒ³è¦é‡ç”¨æç¤ºçš„éƒ¨åˆ†å†…å®¹æ—¶ï¼Œè¯¥ç±»éå¸¸æœ‰ç”¨</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.prompts <span style="color:#f92672">import</span> PipelinePromptTemplate, PromptTemplate
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>full_prompt <span style="color:#f92672">=</span> PromptTemplate<span style="color:#f92672">.</span>from_template(
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span><span style="color:#e6db74">{introduction}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74"></span><span style="color:#e6db74">{example}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74"></span><span style="color:#e6db74">{start}</span><span style="color:#e6db74">&#34;&#34;&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>introduction_prompt <span style="color:#f92672">=</span> PromptTemplate<span style="color:#f92672">.</span>from_template(<span style="color:#e6db74">&#34;You are impersonating </span><span style="color:#e6db74">{person}</span><span style="color:#e6db74">.&#34;</span>)
</span></span><span style="display:flex;"><span> 
</span></span><span style="display:flex;"><span>example_prompt <span style="color:#f92672">=</span> PromptTemplate<span style="color:#f92672">.</span>from_template(
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;Here&#39;s an example of an interaction:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Q: </span><span style="color:#e6db74">{example_q}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">A: </span><span style="color:#e6db74">{example_a}</span><span style="color:#e6db74">&#34;&#34;&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>start_prompt <span style="color:#f92672">=</span> PromptTemplate<span style="color:#f92672">.</span>from_template(
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;Now, do this for real!
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Q: </span><span style="color:#e6db74">{input}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">A:&#34;&#34;&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>input_prompts <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;introduction&#34;</span>, introduction_prompt),
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;example&#34;</span>, example_prompt),
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;start&#34;</span>, start_prompt),
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>pipeline_prompt <span style="color:#f92672">=</span> PipelinePromptTemplate(
</span></span><span style="display:flex;"><span>    final_prompt<span style="color:#f92672">=</span>full_prompt, pipeline_prompts<span style="color:#f92672">=</span>input_prompts
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pipeline_prompt<span style="color:#f92672">.</span>input_variables
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(
</span></span><span style="display:flex;"><span>    pipeline_prompt<span style="color:#f92672">.</span>format(
</span></span><span style="display:flex;"><span>        person<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Elon Musk&#34;</span>,
</span></span><span style="display:flex;"><span>        example_q<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;What&#39;s your favorite car?&#34;</span>,
</span></span><span style="display:flex;"><span>        example_a<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Tesla&#34;</span>,
</span></span><span style="display:flex;"><span>        input<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;What&#39;s your favorite social media site?&#34;</span>,
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h2 id="æ ¼å¼åŒ–è¾“å‡º">æ ¼å¼åŒ–è¾“å‡º<a hidden class="anchor" aria-hidden="true" href="#æ ¼å¼åŒ–è¾“å‡º">#</a></h2>
<p><strong>æ–¹æ³•ä¸€ï¼šä½¿ç”¨pydanticï¼Œè¿”å›ä¸€ä¸ªpydanticå¯¹è±¡ã€‚å¯ä»¥æ ¹æ®è¾“å…¥å†…å®¹è‡ªåŠ¨é€‰æ‹©è¾“å‡ºæ ¼å¼</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> typing <span style="color:#f92672">import</span> Optional
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> typing <span style="color:#f92672">import</span> Union
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.pydantic_v1 <span style="color:#f92672">import</span> BaseModel, Field
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Joke</span>(BaseModel):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Joke to tell user.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    setup: str <span style="color:#f92672">=</span> Field(description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;The setup of the joke&#34;</span>)
</span></span><span style="display:flex;"><span>    punchline: str <span style="color:#f92672">=</span> Field(description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;The punchline to the joke&#34;</span>)
</span></span><span style="display:flex;"><span>    rating: Optional[int] <span style="color:#f92672">=</span> Field(description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;How funny the joke is, from 1 to 10&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ConversationalResponse</span>(BaseModel):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Respond in a conversational manner. Be kind and helpful.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    response: str <span style="color:#f92672">=</span> Field(description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;A conversational response to the user&#39;s query&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># è®©æ¨¡å‹ä»å¤šä¸ªæ¨¡å¼ä¸­è¿›è¡Œè‡ªåŠ¨é€‰æ‹©</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Response</span>(BaseModel):
</span></span><span style="display:flex;"><span>    output: Union[Joke, ConversationalResponse]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>structured_llm <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>with_structured_output(Response)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>structured_llm<span style="color:#f92672">.</span>invoke(<span style="color:#e6db74">&#34;How are you today?&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e">#structured_llm.invoke(&#34;Tell me a joke about cats&#34;)</span>
</span></span></code></pre></div><p><strong>æ–¹æ³•äºŒï¼šJSON Schemaï¼Œè¿”å›ä¸€ä¸ªå­—å…¸</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>json_schema <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;title&#34;</span>: <span style="color:#e6db74">&#34;joke&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;description&#34;</span>: <span style="color:#e6db74">&#34;Joke to tell user.&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;object&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;properties&#34;</span>: {
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;setup&#34;</span>: {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;description&#34;</span>: <span style="color:#e6db74">&#34;The setup of the joke&#34;</span>,
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;punchline&#34;</span>: {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;string&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;description&#34;</span>: <span style="color:#e6db74">&#34;The punchline to the joke&#34;</span>,
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;rating&#34;</span>: {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;integer&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;description&#34;</span>: <span style="color:#e6db74">&#34;How funny the joke is, from 1 to 10&#34;</span>,
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;required&#34;</span>: [<span style="color:#e6db74">&#34;setup&#34;</span>, <span style="color:#e6db74">&#34;punchline&#34;</span>],
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>structured_llm <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>with_structured_output(json_schema)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>structured_llm<span style="color:#f92672">.</span>invoke(<span style="color:#e6db74">&#34;Tell me a joke about cats&#34;</span>)
</span></span></code></pre></div><p><strong>æ–¹æ³•ä¸‰ï¼šå°‘é‡ç¤ºä¾‹ï¼ˆfew-shotï¼‰</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.prompts <span style="color:#f92672">import</span> ChatPromptTemplate
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>system <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;&#34;You are a hilarious comedian. Your specialty is knock-knock jokes. </span><span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span><span style="color:#e6db74">Return a joke which has the setup (the response to &#34;Who&#39;s there?&#34;) and the final punchline (the response to &#34;&lt;setup&gt; who?&#34;).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Here are some examples of jokes:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">example_user: Tell me a joke about planes
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">example_assistant: {{&#34;setup&#34;: &#34;Why don&#39;t planes ever get tired?&#34;, &#34;punchline&#34;: &#34;Because they have rest wings!&#34;, &#34;rating&#34;: 2}}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">example_user: Tell me another joke about planes
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">example_assistant: {{&#34;setup&#34;: &#34;Cargo&#34;, &#34;punchline&#34;: &#34;Cargo &#39;vroom vroom&#39;, but planes go &#39;zoom zoom&#39;!&#34;, &#34;rating&#34;: 10}}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">example_user: Now about caterpillars
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">example_assistant: {{&#34;setup&#34;: &#34;Caterpillar&#34;, &#34;punchline&#34;: &#34;Caterpillar really slow, but watch me turn into a butterfly and steal the show!&#34;, &#34;rating&#34;: 5}}&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> ChatPromptTemplate<span style="color:#f92672">.</span>from_messages([(<span style="color:#e6db74">&#34;system&#34;</span>, system), (<span style="color:#e6db74">&#34;human&#34;</span>, <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{input}</span><span style="color:#e6db74">&#34;</span>)])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>few_shot_structured_llm <span style="color:#f92672">=</span> prompt <span style="color:#f92672">|</span> model
</span></span><span style="display:flex;"><span>few_shot_structured_llm<span style="color:#f92672">.</span>invoke(<span style="color:#e6db74">&#34;what&#39;s something funny about woodpeckers&#34;</span>)
</span></span></code></pre></div><p>**æ–¹æ³•å››ï¼š**å¹¶éæ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒ.with_structured_output()ï¼Œå› æ­¤ä½¿ç”¨å†…ç½®å‡½æ•°PydanticOutputParseræ¥è§£ææç¤ºä¸ç»™å®šçš„ Pydantic æ¨¡å¼åŒ¹é…çš„llmçš„è¾“å‡ºåˆ°Promptä¸­ã€‚</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> typing <span style="color:#f92672">import</span> List
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.output_parsers <span style="color:#f92672">import</span> PydanticOutputParser
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.prompts <span style="color:#f92672">import</span> ChatPromptTemplate
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.pydantic_v1 <span style="color:#f92672">import</span> BaseModel, Field
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Person</span>(BaseModel):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Information about a person.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    name: str <span style="color:#f92672">=</span> Field(<span style="color:#f92672">...</span>, description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;The name of the person&#34;</span>)
</span></span><span style="display:flex;"><span>    height_in_meters: float <span style="color:#f92672">=</span> Field(
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">...</span>, description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;The height of the person expressed in meters.&#34;</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">People</span>(BaseModel):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Identifying information about all people in a text.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    people: List[Person]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Set up a parser</span>
</span></span><span style="display:flex;"><span>parser <span style="color:#f92672">=</span> PydanticOutputParser(pydantic_object<span style="color:#f92672">=</span>People)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Prompt</span>
</span></span><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> ChatPromptTemplate<span style="color:#f92672">.</span>from_messages(
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        (
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;system&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;Answer the user query. Wrap the output in `json` tags</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{format_instructions}</span><span style="color:#e6db74">&#34;</span>,
</span></span><span style="display:flex;"><span>        ),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;human&#34;</span>, <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{query}</span><span style="color:#e6db74">&#34;</span>),
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>)<span style="color:#f92672">.</span>partial(format_instructions<span style="color:#f92672">=</span>parser<span style="color:#f92672">.</span>get_format_instructions())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>query <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Anna is 23 years old and she is 6 feet tall&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(prompt<span style="color:#f92672">.</span>invoke(query)<span style="color:#f92672">.</span>to_string())
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># å†æ¬¡ä½¿ç”¨parserï¼Œåˆå¯ä»¥æŠŠè¾“å‡ºè½¬æ¢æˆParserå¯¹è±¡</span>
</span></span><span style="display:flex;"><span>chain <span style="color:#f92672">=</span> prompt <span style="color:#f92672">|</span> model <span style="color:#f92672">|</span> parser
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>chain<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;query&#34;</span>: query})
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Prompt</span>
</span></span><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> ChatPromptTemplate<span style="color:#f92672">.</span>from_messages(
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        (
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;system&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;Answer the user query. Wrap the output in `json` tags</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{schema}</span><span style="color:#e6db74">&#34;</span>,
</span></span><span style="display:flex;"><span>        ),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;human&#34;</span>, <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{query}</span><span style="color:#e6db74">&#34;</span>),
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>)<span style="color:#f92672">.</span>partial(schema<span style="color:#f92672">=</span>People<span style="color:#f92672">.</span>schema())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>query <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Anna is 23 years old and she is 6 feet tall&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(prompt<span style="color:#f92672">.</span>format_prompt(query<span style="color:#f92672">=</span>query)<span style="color:#f92672">.</span>to_string())
</span></span></code></pre></div><h2 id="æ„å»ºchain">æ„å»ºchain<a hidden class="anchor" aria-hidden="true" href="#æ„å»ºchain">#</a></h2>
<p><strong>1. LangChain Expression Language (LCEL): |</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.output_parsers <span style="color:#f92672">import</span> StrOutputParser
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.prompts <span style="color:#f92672">import</span> ChatPromptTemplate
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> ChatPromptTemplate<span style="color:#f92672">.</span>from_template(<span style="color:#e6db74">&#34;tell me a joke about </span><span style="color:#e6db74">{user_input}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>chain <span style="color:#f92672">=</span> prompt <span style="color:#f92672">|</span> model <span style="color:#f92672">|</span> StrOutputParser()
</span></span><span style="display:flex;"><span>chain<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;user_input&#34;</span>: <span style="color:#e6db74">&#34;bears&#34;</span>})
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># ç¬¬äºŒç§æ–¹æ³•</span>
</span></span><span style="display:flex;"><span>composed_chain <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;user_input&#34;</span>: chain} <span style="color:#f92672">|</span> prompt <span style="color:#f92672">|</span> model <span style="color:#f92672">|</span> StrOutputParser()
</span></span><span style="display:flex;"><span>composed_chain<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;user_input&#34;</span>: <span style="color:#e6db74">&#34;bears&#34;</span>})
</span></span><span style="display:flex;"><span><span style="color:#75715e">#composed_chain.invoke({&#34;bears&#34;}) #åªæœ‰ä¸€ä¸ªå‚æ•°è¿˜å¯ä»¥ç›´æ¥ä¼ </span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># ç¬¬ä¸‰ç§æ–¹æ³•ï¼ŒRunnablePassthroughç›´æ¥è¿‡å»åŸå‚æ•°</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.runnables <span style="color:#f92672">import</span> RunnablePassthrough
</span></span><span style="display:flex;"><span>Runnable_chain <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;user_input&#34;</span>:  RunnablePassthrough()} <span style="color:#f92672">|</span> prompt <span style="color:#f92672">|</span> model <span style="color:#f92672">|</span> StrOutputParser()
</span></span><span style="display:flex;"><span>Runnable_chain<span style="color:#f92672">.</span>invoke(<span style="color:#e6db74">&#34;bears&#34;</span>)
</span></span></code></pre></div><p><strong>2. pipe()</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.runnables <span style="color:#f92672">import</span> RunnableParallel
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>composed_chain_with_pipe <span style="color:#f92672">=</span> RunnableParallel({<span style="color:#e6db74">&#34;user_input&#34;</span>: chain})<span style="color:#f92672">.</span>pipe(
</span></span><span style="display:flex;"><span>    prompt, model, StrOutputParser()
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>composed_chain_with_pipe<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;user_input&#34;</span>: <span style="color:#e6db74">&#34;bears&#34;</span>})
</span></span></code></pre></div><h2 id="æ·»åŠ å†å²è®°å½•">æ·»åŠ å†å²è®°å½•<a hidden class="anchor" aria-hidden="true" href="#æ·»åŠ å†å²è®°å½•">#</a></h2>
<p><strong>1.ä½¿ç”¨SQLiteæ•°æ®åº“ä¿å­˜</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_community.chat_message_histories <span style="color:#f92672">import</span> SQLChatMessageHistory
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># session_idæ˜¯è¿™äº›è¾“å…¥æ¶ˆæ¯å¯¹åº”çš„ä¼šè¯ï¼ˆå¯¹è¯ï¼‰çº¿ç¨‹çš„æ ‡è¯†ç¬¦ã€‚è¿™å…è®¸æ‚¨åŒæ—¶ç»´æŠ¤åŒä¸€é“¾ä¸­çš„å¤šä¸ªå¯¹è¯/çº¿ç¨‹ã€‚</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_session_history</span>(session_id):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> SQLChatMessageHistory(session_id, <span style="color:#e6db74">&#34;sqlite:///memory.db&#34;</span>)  <span style="color:#75715e"># å†å²å¯¹è¯ä¼šä¿å­˜åœ¨å½“å‰é¡¹ç›®çš„memory.dbæ–‡ä»¶é‡Œ</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.messages <span style="color:#f92672">import</span> HumanMessage
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.prompts <span style="color:#f92672">import</span> ChatPromptTemplate, MessagesPlaceholder
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.runnables.history <span style="color:#f92672">import</span> RunnableWithMessageHistory
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> ChatPromptTemplate<span style="color:#f92672">.</span>from_messages(
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        (
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;system&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;You&#39;re an assistant who speaks in </span><span style="color:#e6db74">{language}</span><span style="color:#e6db74">. Respond in 20 words or fewer&#34;</span>,
</span></span><span style="display:flex;"><span>        ),
</span></span><span style="display:flex;"><span>        MessagesPlaceholder(variable_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;history&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;human&#34;</span>, <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{input}</span><span style="color:#e6db74">&#34;</span>),
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>runnable <span style="color:#f92672">=</span> prompt <span style="color:#f92672">|</span> model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>runnable_with_history <span style="color:#f92672">=</span> RunnableWithMessageHistory(
</span></span><span style="display:flex;"><span>    runnable,
</span></span><span style="display:flex;"><span>    get_session_history,
</span></span><span style="display:flex;"><span>    input_messages_key<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;input&#34;</span>,
</span></span><span style="display:flex;"><span>    history_messages_key<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;history&#34;</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>runnable_with_history<span style="color:#f92672">.</span>invoke(
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;language&#34;</span>: <span style="color:#e6db74">&#34;italian&#34;</span>, <span style="color:#e6db74">&#34;input&#34;</span>: <span style="color:#e6db74">&#34;hi im bob!&#34;</span>},
</span></span><span style="display:flex;"><span>    config<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;configurable&#34;</span>: {<span style="color:#e6db74">&#34;session_id&#34;</span>: <span style="color:#e6db74">&#34;2&#34;</span>}},
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸Šä¸‹æ–‡é€šè¿‡æä¾›çš„èŠå¤©å†å²è®°å½•è¿›è¡Œä¿ç•™session_idï¼Œå› æ­¤æ¨¡å‹çŸ¥é“ç”¨æˆ·çš„å§“åã€‚</span>
</span></span><span style="display:flex;"><span>runnable_with_history<span style="color:#f92672">.</span>invoke(
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;language&#34;</span>: <span style="color:#e6db74">&#34;italian&#34;</span>, <span style="color:#e6db74">&#34;input&#34;</span>: <span style="color:#e6db74">&#34;whats my name?&#34;</span>},
</span></span><span style="display:flex;"><span>    config<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;configurable&#34;</span>: {<span style="color:#e6db74">&#34;session_id&#34;</span>: <span style="color:#e6db74">&#34;2&#34;</span>}},
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p><strong>2.ç”¨æˆ·å®šåˆ¶</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.runnables <span style="color:#f92672">import</span> ConfigurableFieldSpec
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_session_history</span>(user_id: str, conversation_id: str):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> SQLChatMessageHistory(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>user_id<span style="color:#e6db74">}</span><span style="color:#e6db74">--</span><span style="color:#e6db74">{</span>conversation_id<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>, <span style="color:#e6db74">&#34;sqlite:///memory.db&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>with_message_history <span style="color:#f92672">=</span> RunnableWithMessageHistory(
</span></span><span style="display:flex;"><span>    runnable,
</span></span><span style="display:flex;"><span>    get_session_history,
</span></span><span style="display:flex;"><span>    input_messages_key<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;input&#34;</span>,
</span></span><span style="display:flex;"><span>    history_messages_key<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;history&#34;</span>,
</span></span><span style="display:flex;"><span>    history_factory_config<span style="color:#f92672">=</span>[
</span></span><span style="display:flex;"><span>        ConfigurableFieldSpec(
</span></span><span style="display:flex;"><span>            id<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;user_id&#34;</span>,
</span></span><span style="display:flex;"><span>            annotation<span style="color:#f92672">=</span>str,
</span></span><span style="display:flex;"><span>            name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;User ID&#34;</span>,
</span></span><span style="display:flex;"><span>            description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Unique identifier for the user.&#34;</span>,
</span></span><span style="display:flex;"><span>            default<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>            is_shared<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>        ),
</span></span><span style="display:flex;"><span>        ConfigurableFieldSpec(
</span></span><span style="display:flex;"><span>            id<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;conversation_id&#34;</span>,
</span></span><span style="display:flex;"><span>            annotation<span style="color:#f92672">=</span>str,
</span></span><span style="display:flex;"><span>            name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Conversation ID&#34;</span>,
</span></span><span style="display:flex;"><span>            description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Unique identifier for the conversation.&#34;</span>,
</span></span><span style="display:flex;"><span>            default<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>            is_shared<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>        ),
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>with_message_history<span style="color:#f92672">.</span>invoke(
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;language&#34;</span>: <span style="color:#e6db74">&#34;italian&#34;</span>, <span style="color:#e6db74">&#34;input&#34;</span>: <span style="color:#e6db74">&#34;hi im bob!&#34;</span>},
</span></span><span style="display:flex;"><span>    config<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;configurable&#34;</span>: {<span style="color:#e6db74">&#34;user_id&#34;</span>: <span style="color:#e6db74">&#34;123&#34;</span>, <span style="color:#e6db74">&#34;conversation_id&#34;</span>: <span style="color:#e6db74">&#34;1&#34;</span>}},
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h2 id="toolsfunction-calling">Toolsï¼ˆfunction callingï¼‰<a hidden class="anchor" aria-hidden="true" href="#toolsfunction-calling">#</a></h2>
<p><strong>1.åˆ›å»ºtoolå’Œè°ƒç”¨</strong></p>
<p>æ–¹å¼ä¸€ï¼š@toolè£…é¥°å™¨+StructuredTool</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.tools <span style="color:#f92672">import</span> tool
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># parse_docstring=Trueå¯¹æ–‡æ¡£å­—ç¬¦ä¸²è¿›è¡Œè§£æã€‚æ³¨æ„ï¼šå‡½æ•°æè¿°å’Œå‚æ•°æè¿°ä¸­é—´è¦ç©ºä¸€è¡Œã€‚</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@tool</span>(<span style="color:#e6db74">&#34;multiplication-tool&#34;</span>, parse_docstring<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, return_direct<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">multiply</span>(a: int, b: int) <span style="color:#f92672">-&gt;</span> int:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Multiply two numbers.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        a: first number.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        b: second number.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> a <span style="color:#f92672">*</span> b
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(multiply<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;a&#34;</span>: <span style="color:#ae81ff">2</span>, <span style="color:#e6db74">&#34;b&#34;</span>: <span style="color:#ae81ff">3</span>}))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Let&#39;s inspect some of the attributes associated with the tool.</span>
</span></span><span style="display:flex;"><span>print(multiply<span style="color:#f92672">.</span>name)
</span></span><span style="display:flex;"><span>print(multiply<span style="color:#f92672">.</span>description)
</span></span><span style="display:flex;"><span>print(multiply<span style="color:#f92672">.</span>args)
</span></span><span style="display:flex;"><span>print(multiply<span style="color:#f92672">.</span>return_direct)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>multiply<span style="color:#f92672">.</span>args_schema<span style="color:#f92672">.</span>schema()
</span></span></code></pre></div><p>æ–¹å¼äºŒï¼š@toolè£…é¥°å™¨+pydantic</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.tools <span style="color:#f92672">import</span> tool
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.pydantic_v1 <span style="color:#f92672">import</span> BaseModel, Field
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ç»™å‚æ•°æ·»åŠ æè¿°</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CalculatorInput</span>(BaseModel):
</span></span><span style="display:flex;"><span>    a: int <span style="color:#f92672">=</span> Field(description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;first number&#34;</span>)
</span></span><span style="display:flex;"><span>    b: int <span style="color:#f92672">=</span> Field(description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;second number&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@tool</span>(<span style="color:#e6db74">&#34;multiplication-tool&#34;</span>, args_schema<span style="color:#f92672">=</span>CalculatorInput, return_direct<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">multiply</span>(a: int, b: int) <span style="color:#f92672">-&gt;</span> int:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Multiply two numbers.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> a <span style="color:#f92672">*</span> b
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(multiply<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;a&#34;</span>: <span style="color:#ae81ff">2</span>, <span style="color:#e6db74">&#34;b&#34;</span>: <span style="color:#ae81ff">3</span>}))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Let&#39;s inspect some of the attributes associated with the tool.</span>
</span></span><span style="display:flex;"><span>print(multiply<span style="color:#f92672">.</span>name)
</span></span><span style="display:flex;"><span>print(multiply<span style="color:#f92672">.</span>description)
</span></span><span style="display:flex;"><span>print(multiply<span style="color:#f92672">.</span>args)
</span></span><span style="display:flex;"><span>print(multiply<span style="color:#f92672">.</span>return_direct)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>multiply<span style="color:#f92672">.</span>args_schema<span style="color:#f92672">.</span>schema()
</span></span></code></pre></div><p>æ–¹å¼ä¸‰ï¼šStructuredTool+pydantic</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.tools <span style="color:#f92672">import</span> StructuredTool
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.pydantic_v1 <span style="color:#f92672">import</span> BaseModel, Field
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># å¯¹å‚æ•°çš„æè¿°</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CalculatorInput</span>(BaseModel):
</span></span><span style="display:flex;"><span>    a: int <span style="color:#f92672">=</span> Field(description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;first number&#34;</span>)
</span></span><span style="display:flex;"><span>    b: int <span style="color:#f92672">=</span> Field(description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;second number&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">multiply</span>(a: int, b: int) <span style="color:#f92672">-&gt;</span> int:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Multiply two numbers.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> a <span style="color:#f92672">*</span> b
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">amultiply</span>(a: int, b: int) <span style="color:#f92672">-&gt;</span> int:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Multiply two numbers.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> a <span style="color:#f92672">*</span> b
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>calculator <span style="color:#f92672">=</span> StructuredTool<span style="color:#f92672">.</span>from_function(
</span></span><span style="display:flex;"><span>    func<span style="color:#f92672">=</span>multiply,
</span></span><span style="display:flex;"><span>    name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Calculator&#34;</span>,
</span></span><span style="display:flex;"><span>    description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;multiply numbers&#34;</span>,
</span></span><span style="display:flex;"><span>    args_schema<span style="color:#f92672">=</span>CalculatorInput,
</span></span><span style="display:flex;"><span>    return_direct<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    coroutine<span style="color:#f92672">=</span> amultiply <span style="color:#75715e">#&lt;- å¦‚æœéœ€è¦ï¼Œæ‚¨ä¹Ÿå¯ä»¥æŒ‡å®šå¼‚æ­¥æ–¹æ³•</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(calculator<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;a&#34;</span>: <span style="color:#ae81ff">2</span>, <span style="color:#e6db74">&#34;b&#34;</span>: <span style="color:#ae81ff">3</span>}))
</span></span><span style="display:flex;"><span>print(<span style="color:#66d9ef">await</span> calculator<span style="color:#f92672">.</span>ainvoke({<span style="color:#e6db74">&#34;a&#34;</span>: <span style="color:#ae81ff">4</span>, <span style="color:#e6db74">&#34;b&#34;</span>: <span style="color:#ae81ff">3</span>}))
</span></span><span style="display:flex;"><span>print(calculator<span style="color:#f92672">.</span>name)
</span></span><span style="display:flex;"><span>print(calculator<span style="color:#f92672">.</span>description)
</span></span><span style="display:flex;"><span>print(calculator<span style="color:#f92672">.</span>args)
</span></span></code></pre></div><p>æ–¹å¼å››ï¼šå°†chainä½œä¸ºå·¥å…·+.as_tool()</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.language_models <span style="color:#f92672">import</span> GenericFakeChatModel
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.output_parsers <span style="color:#f92672">import</span> StrOutputParser
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.prompts <span style="color:#f92672">import</span> ChatPromptTemplate
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> ChatPromptTemplate<span style="color:#f92672">.</span>from_messages(
</span></span><span style="display:flex;"><span>    [(<span style="color:#e6db74">&#34;human&#34;</span>, <span style="color:#e6db74">&#34;Hello. Please respond in the style of </span><span style="color:#e6db74">{answer_style}</span><span style="color:#e6db74">.&#34;</span>)]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Placeholder LLM</span>
</span></span><span style="display:flex;"><span>llm <span style="color:#f92672">=</span> GenericFakeChatModel(messages<span style="color:#f92672">=</span>iter([<span style="color:#e6db74">&#34;hello matey&#34;</span>]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>chain <span style="color:#f92672">=</span> prompt <span style="color:#f92672">|</span> llm <span style="color:#f92672">|</span> StrOutputParser()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>as_tool <span style="color:#f92672">=</span> chain<span style="color:#f92672">.</span>as_tool(
</span></span><span style="display:flex;"><span>    name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Style responder&#34;</span>, description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Description of when to use tool.&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>as_tool<span style="color:#f92672">.</span>args
</span></span></code></pre></div><p>æ–¹å¼äº”ï¼šå­ç±»åŒ–åˆ›å»ºå·¥å…·ï¼ŒBaseModel+BaseTool</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> typing <span style="color:#f92672">import</span> Optional, Type
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.pydantic_v1 <span style="color:#f92672">import</span> BaseModel
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.callbacks <span style="color:#f92672">import</span> (
</span></span><span style="display:flex;"><span>    AsyncCallbackManagerForToolRun,
</span></span><span style="display:flex;"><span>    CallbackManagerForToolRun,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.tools <span style="color:#f92672">import</span> BaseTool
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CalculatorInput</span>(BaseModel):
</span></span><span style="display:flex;"><span>    a: int <span style="color:#f92672">=</span> Field(description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;first number&#34;</span>)
</span></span><span style="display:flex;"><span>    b: int <span style="color:#f92672">=</span> Field(description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;second number&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CustomCalculatorTool</span>(BaseTool):
</span></span><span style="display:flex;"><span>    name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Calculator&#34;</span>
</span></span><span style="display:flex;"><span>    description <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;useful for when you need to answer questions about math&#34;</span>
</span></span><span style="display:flex;"><span>    args_schema: Type[BaseModel] <span style="color:#f92672">=</span> CalculatorInput
</span></span><span style="display:flex;"><span>    return_direct: bool <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_run</span>(
</span></span><span style="display:flex;"><span>        self, a: int, b: int, run_manager: Optional[CallbackManagerForToolRun] <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>    ) <span style="color:#f92672">-&gt;</span> str:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Use the tool.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> a <span style="color:#f92672">*</span> b
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_arun</span>(
</span></span><span style="display:flex;"><span>        self,
</span></span><span style="display:flex;"><span>        a: int,
</span></span><span style="display:flex;"><span>        b: int,
</span></span><span style="display:flex;"><span>        run_manager: Optional[AsyncCallbackManagerForToolRun] <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>    ) <span style="color:#f92672">-&gt;</span> str:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Use the tool asynchronously.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># If the calculation is cheap, you can just delegate to the sync implementation</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># as shown below.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># If the sync calculation is expensive, you should delete the entire _arun method.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># LangChain will automatically provide a better implementation that will</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># kick off the task in a thread to make sure it doesn&#39;t block other async code.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>_run(a, b, run_manager<span style="color:#f92672">=</span>run_manager<span style="color:#f92672">.</span>get_sync())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>multiply <span style="color:#f92672">=</span> CustomCalculatorTool()
</span></span><span style="display:flex;"><span>print(multiply<span style="color:#f92672">.</span>name)
</span></span><span style="display:flex;"><span>print(multiply<span style="color:#f92672">.</span>description)
</span></span><span style="display:flex;"><span>print(multiply<span style="color:#f92672">.</span>args)
</span></span><span style="display:flex;"><span>print(multiply<span style="color:#f92672">.</span>return_direct)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(multiply<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;a&#34;</span>: <span style="color:#ae81ff">2</span>, <span style="color:#e6db74">&#34;b&#34;</span>: <span style="color:#ae81ff">3</span>}))
</span></span><span style="display:flex;"><span>print(<span style="color:#66d9ef">await</span> multiply<span style="color:#f92672">.</span>ainvoke({<span style="color:#e6db74">&#34;a&#34;</span>: <span style="color:#ae81ff">2</span>, <span style="color:#e6db74">&#34;b&#34;</span>: <span style="color:#ae81ff">3</span>}))
</span></span></code></pre></div><p><strong>2.åœ¨chatLLMæ‰§è¡Œtoolï¼ˆfunction callingï¼‰</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># ä½¿ç”¨è¯¥.bind_tools()æ–¹æ³•æ¥å¤„ç†è½¬æ¢ Multiplyä¸ºæ¨¡å‹çš„æ­£ç¡®æ ¼å¼ï¼Œç„¶åç»‘å®šå®ƒï¼ˆå³ï¼Œæ¯æ¬¡è°ƒç”¨æ¨¡å‹æ—¶ä¼ é€’å®ƒï¼‰</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.tools <span style="color:#f92672">import</span> tool
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ç®€å•çš„å®šä¹‰ä¸¤ä¸ªtool</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@tool</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">add</span>(a: int, b: int) <span style="color:#f92672">-&gt;</span> int:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Adds a and b.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> a <span style="color:#f92672">+</span> b
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@tool</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">multiply</span>(a: int, b: int) <span style="color:#f92672">-&gt;</span> int:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Multiplies a and b.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> a <span style="color:#f92672">*</span> b
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tools <span style="color:#f92672">=</span> [add, multiply]
</span></span><span style="display:flex;"><span>llm_with_tools <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>bind_tools(tools)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.messages <span style="color:#f92672">import</span> AIMessage, HumanMessage, SystemMessage, ToolMessage
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># å¯¹äºè¿™ç§promptçš„è®¾è®¡ï¼Œä¸é€‚åˆè¿ç»­å¯¹è¯</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">llm_tool_calling</span>(query):
</span></span><span style="display:flex;"><span>    prompt <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>        SystemMessage(content<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;You are a nice Mathematician.&#34;</span>) <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>        HumanMessage(content<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Can you help me with some math?&#34;</span>) <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>        AIMessage(content<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Sure, tell me your question.&#34;</span>) <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{user_input}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    chain <span style="color:#f92672">=</span> prompt <span style="color:#f92672">|</span> llm_with_tools
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    ai_msg <span style="color:#f92672">=</span> chain<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;user_input&#34;</span>: query})
</span></span><span style="display:flex;"><span>    prompt<span style="color:#f92672">.</span>append(ai_msg)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#prompt.format_messages(user_input=query)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> ai_msg<span style="color:#f92672">.</span>content <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;&#34;</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Simple sequential tool calling helper.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        tool_map <span style="color:#f92672">=</span> {tool<span style="color:#f92672">.</span>name: tool <span style="color:#66d9ef">for</span> tool <span style="color:#f92672">in</span> tools}
</span></span><span style="display:flex;"><span>        tool_calls <span style="color:#f92672">=</span> ai_msg<span style="color:#f92672">.</span>tool_calls<span style="color:#f92672">.</span>copy()  <span style="color:#75715e"># è¿”å›ä¸€ä¸ªå­—å…¸åˆ—è¡¨</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> tool_call <span style="color:#f92672">in</span> tool_calls:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># å¯¹@toolå‡½æ•°æ‰§è¡Œ.invoke(tool_call)è¿”å›çš„æ˜¯ToolMessageå¯¹è±¡,æ‰§è¡Œtool_call[&#34;args&#34;]è¿”å›å‡½æ•°ç»“æœ</span>
</span></span><span style="display:flex;"><span>            tool_msg <span style="color:#f92672">=</span> tool_map[tool_call[<span style="color:#e6db74">&#34;name&#34;</span>]]<span style="color:#f92672">.</span>invoke(tool_call)
</span></span><span style="display:flex;"><span>            prompt<span style="color:#f92672">.</span>append(tool_msg)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        result <span style="color:#f92672">=</span> chain<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;user_input&#34;</span>: query})<span style="color:#f92672">.</span>content
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        result <span style="color:#f92672">=</span> ai_msg<span style="color:#f92672">.</span>content
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> result
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ç»‘å®štoolsçš„LLMä¼šè‡ªè¡Œåˆ¤æ–­æ˜¯å¦è°ƒç”¨toolï¼Œä»¥åŠè°ƒç”¨å“ªå‡ ä¸ªå‡ ä¸ªtoolã€‚æ‰€ä»¥åœ¨è¿™ä¹‹åå¯ä»¥åŠ ä¸€ä¸ªåˆ¤æ–­æ—¶å€™è¦æ‰§è¡Œtool</span>
</span></span><span style="display:flex;"><span>query <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;What is 3 * 12?&#34;</span>
</span></span><span style="display:flex;"><span>query <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;GPTæ˜¯ä»€ä¹ˆ?&#34;</span>
</span></span><span style="display:flex;"><span>query <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;What is 3 * 12? Also, what is 11 + 49?&#34;</span>
</span></span><span style="display:flex;"><span>print(llm_tool_calling(query))
</span></span></code></pre></div><p><strong>3.ä½¿ç”¨å†…ç½®å·¥å…·å’Œå·¥å…·åŒ…</strong></p>
<p>å„å·¥å…·åŒ…çš„ä½¿ç”¨æ–‡æ¡£ï¼šhttps://python.langchain.com/v0.2/docs/integrations/tools/</p>
<p>ä¸‹é¢ä»¥è°·æ­Œæœç´¢ä¸ºä¾‹ï¼š é¦–å…ˆï¼Œæ‚¨éœ€è¦è®¾ç½®æ­£ç¡®çš„ API å¯†é’¥å’Œç¯å¢ƒå˜é‡ã€‚è¦è¿›è¡Œè®¾ç½®ï¼Œè¯·åœ¨ Google Cloud å‡­æ®æ§åˆ¶å° ( <a href="https://console.cloud.google.com/apis/credentials">https://console.cloud.google.com/apis/credentials</a> ) ä¸­åˆ›å»º GOOGLE_API_KEYï¼Œå¹¶ä½¿ç”¨å¯ç¼–ç¨‹æœç´¢å¼•æ“ ( <a href="https://programmablesearchengine.google.com/controlpanel/create">https://programmablesearchengine.google.com/controlpanel/create</a> ) åˆ›å»º GOOGLE_CSE_IDã€‚æ¥ä¸‹æ¥ï¼Œæœ€å¥½æŒ‰ç…§æ­¤å¤„ï¼ˆhttps://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-searchï¼‰çš„è¯´æ˜è¿›è¡Œæ“ä½œã€‚</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">!</span>pip install google<span style="color:#f92672">-</span>api<span style="color:#f92672">-</span>python<span style="color:#f92672">-</span>client
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># æ£€æŸ¥æ˜¯å¦å¯ä»¥è°ƒç”¨Google API</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> googleapiclient.discovery <span style="color:#f92672">import</span> build
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>my_api_key <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;GOOGLE_API_KEY&#34;</span>)
</span></span><span style="display:flex;"><span>my_cse_id <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;GOOGLE_API_KEY&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">google_search</span>(search_term, api_key, cse_id, <span style="color:#f92672">**</span>kwargs):
</span></span><span style="display:flex;"><span>    service <span style="color:#f92672">=</span> build(<span style="color:#e6db74">&#34;customsearch&#34;</span>, <span style="color:#e6db74">&#34;v1&#34;</span>, developerKey<span style="color:#f92672">=</span>api_key)
</span></span><span style="display:flex;"><span>    res <span style="color:#f92672">=</span> service<span style="color:#f92672">.</span>cse()<span style="color:#f92672">.</span>list(q<span style="color:#f92672">=</span>search_term, cx<span style="color:#f92672">=</span>cse_id, <span style="color:#f92672">**</span>kwargs)<span style="color:#f92672">.</span>execute()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> res[<span style="color:#e6db74">&#39;items&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>results <span style="color:#f92672">=</span> google_search(<span style="color:#e6db74">&#39;&#34;god is a woman&#34; &#34;thank you next&#34; &#34;7 rings&#34;&#39;</span>, my_api_key, my_cse_id, num<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> result <span style="color:#f92672">in</span> results:
</span></span><span style="display:flex;"><span>    print(result)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">!</span>pip install <span style="color:#f92672">-</span>U langchain<span style="color:#f92672">-</span>google<span style="color:#f92672">-</span>community
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#34;GOOGLE_CSE_ID&#34;</span>] <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;GOOGLE_CSE_ID&#34;</span>)
</span></span><span style="display:flex;"><span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#34;GOOGLE_API_KEY&#34;</span>] <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;GOOGLE_API_KEY&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_google_community <span style="color:#f92672">import</span> GoogleSearchAPIWrapper
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.tools <span style="color:#f92672">import</span> Tool
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>search <span style="color:#f92672">=</span> GoogleSearchAPIWrapper()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tool <span style="color:#f92672">=</span> Tool(
</span></span><span style="display:flex;"><span>    name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;google_search&#34;</span>,
</span></span><span style="display:flex;"><span>    description<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Search Google for recent results.&#34;</span>,
</span></span><span style="display:flex;"><span>    func<span style="color:#f92672">=</span>search<span style="color:#f92672">.</span>run,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tool<span style="color:#f92672">.</span>run(<span style="color:#e6db74">&#34;Obama&#39;s first name?&#34;</span>)
</span></span></code></pre></div><h2 id="rag">RAG<a hidden class="anchor" aria-hidden="true" href="#rag">#</a></h2>
<p><strong>1.SemanticSimilarityExampleSelector</strong>:ä½¿ç”¨åµŒå…¥æ¨¡å‹æ¥è®¡ç®—è¾“å…¥ä¸å°‘é‡æ ·æœ¬ç¤ºä¾‹ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶ä½¿ç”¨å‘é‡å­˜å‚¨æ¥æ‰§è¡Œæœ€è¿‘é‚»æœç´¢.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_chroma <span style="color:#f92672">import</span> Chroma
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.example_selectors <span style="color:#f92672">import</span> SemanticSimilarityExampleSelector
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_openai <span style="color:#f92672">import</span> OpenAIEmbeddings
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>example_selector <span style="color:#f92672">=</span> SemanticSimilarityExampleSelector<span style="color:#f92672">.</span>from_examples(
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># This is the list of examples available to select from.</span>
</span></span><span style="display:flex;"><span>    examples,
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># This is the embedding class used to produce embeddings which are used to measure semantic similarity.</span>
</span></span><span style="display:flex;"><span>    OpenAIEmbeddings(),
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># This is the VectorStore class that is used to store the embeddings and do a similarity search over.</span>
</span></span><span style="display:flex;"><span>    Chroma,
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># This is the number of examples to produce.</span>
</span></span><span style="display:flex;"><span>    k<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Select the most similar example to the input.</span>
</span></span><span style="display:flex;"><span>question <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Who was the father of Mary Ball Washington?&#34;</span>
</span></span><span style="display:flex;"><span>selected_examples <span style="color:#f92672">=</span> example_selector<span style="color:#f92672">.</span>select_examples({<span style="color:#e6db74">&#34;question&#34;</span>: question})
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Examples most similar to the input: </span><span style="color:#e6db74">{</span>question<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> example <span style="color:#f92672">in</span> selected_examples:
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> example<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>k<span style="color:#e6db74">}</span><span style="color:#e6db74">: </span><span style="color:#e6db74">{</span>v<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> FewShotPromptTemplate(
</span></span><span style="display:flex;"><span>    example_selector<span style="color:#f92672">=</span>example_selector, 
</span></span><span style="display:flex;"><span>    example_prompt<span style="color:#f92672">=</span>example_prompt,
</span></span><span style="display:flex;"><span>    suffix<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Question: </span><span style="color:#e6db74">{input}</span><span style="color:#e6db74">&#34;</span>,
</span></span><span style="display:flex;"><span>    input_variables<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;input&#34;</span>],
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(
</span></span><span style="display:flex;"><span>    prompt<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;input&#34;</span>: <span style="color:#e6db74">&#34;Who was the father of Mary Ball Washington?&#34;</span>})<span style="color:#f92672">.</span>to_string()
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_chroma <span style="color:#f92672">import</span> Chroma
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.example_selectors <span style="color:#f92672">import</span> SemanticSimilarityExampleSelector
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_openai <span style="color:#f92672">import</span> OpenAIEmbeddings
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>examples <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;input&#34;</span>: <span style="color:#e6db74">&#34;2 ğŸ¦œ 2&#34;</span>, <span style="color:#e6db74">&#34;output&#34;</span>: <span style="color:#e6db74">&#34;4&#34;</span>},
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;input&#34;</span>: <span style="color:#e6db74">&#34;2 ğŸ¦œ 3&#34;</span>, <span style="color:#e6db74">&#34;output&#34;</span>: <span style="color:#e6db74">&#34;5&#34;</span>},
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;input&#34;</span>: <span style="color:#e6db74">&#34;2 ğŸ¦œ 4&#34;</span>, <span style="color:#e6db74">&#34;output&#34;</span>: <span style="color:#e6db74">&#34;6&#34;</span>},
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;input&#34;</span>: <span style="color:#e6db74">&#34;What did the cow say to the moon?&#34;</span>, <span style="color:#e6db74">&#34;output&#34;</span>: <span style="color:#e6db74">&#34;nothing at all&#34;</span>},
</span></span><span style="display:flex;"><span>    {
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;input&#34;</span>: <span style="color:#e6db74">&#34;Write me a poem about the moon&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;output&#34;</span>: <span style="color:#e6db74">&#34;One for the moon, and one for me, who are we to talk about the moon?&#34;</span>,
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>to_vectorize <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34; &#34;</span><span style="color:#f92672">.</span>join(example<span style="color:#f92672">.</span>values()) <span style="color:#66d9ef">for</span> example <span style="color:#f92672">in</span> examples]
</span></span><span style="display:flex;"><span>embeddings <span style="color:#f92672">=</span> OpenAIEmbeddings()
</span></span><span style="display:flex;"><span>vectorstore <span style="color:#f92672">=</span> Chroma<span style="color:#f92672">.</span>from_texts(to_vectorize, embeddings, metadatas<span style="color:#f92672">=</span>examples)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>example_selector <span style="color:#f92672">=</span> SemanticSimilarityExampleSelector(
</span></span><span style="display:flex;"><span>    vectorstore<span style="color:#f92672">=</span>vectorstore,
</span></span><span style="display:flex;"><span>    k<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The prompt template will load examples by passing the input do the `select_examples` method</span>
</span></span><span style="display:flex;"><span>example_selector<span style="color:#f92672">.</span>select_examples({<span style="color:#e6db74">&#34;input&#34;</span>: <span style="color:#e6db74">&#34;horse&#34;</span>})
</span></span></code></pre></div><p>æœªå®Œå¾…ç»­ï¼ï¼ï¼</p>
<p><strong>2.LlamaIndex</strong></p>
<p>æœªå®Œå¾…ç»­ï¼ï¼ï¼</p>
<h1 id="å…¶ä»–åŠŸèƒ½">å…¶ä»–åŠŸèƒ½<a hidden class="anchor" aria-hidden="true" href="#å…¶ä»–åŠŸèƒ½">#</a></h1>
<p>æœªå®Œå¾…ç»­ï¼ï¼ï¼</p>


        </div>
        <div class="post-reward">
            <div style="padding: 0 0 0 0; margin: 0 0 0 0; width: 100%; font-size:16px; text-align: center;">
                <div id="QR" style="opacity: 0;">
                    <div id="wechat" style="display: inline-block">
                        <a class="fancybox" rel="group">
                            <img id="wechat_qr" src="https://Aliga123.github.io/img/wechat_pay.jpg" alt="wechat_pay"></a>
                        <p>å¾®ä¿¡</p>
                    </div>
                    <div id="alipay" style="display: inline-block">
                        <a class="fancybox" rel="group">
                            <img id="alipay_qr" src="https://Aliga123.github.io/img/alipay.jpg" alt="alipay"></a>
                        <p>æ”¯ä»˜å®</p>
                    </div>
                </div>
                <button id="rewardButton"
                        onclick="
                    var qr = document.getElementById('QR');
                    if (qr.style.opacity === '0') {
                        qr.style.opacity='1';
                    } else {
                        qr.style.opacity='0'
                    }"
                >
                    <span>ğŸ§§ é¼“åŠ±</span>
                </button>
            </div>
        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://Aliga123.github.io/posts/ai-llm/function-calling/">
    <span class="title">Â« ä¸Šä¸€é¡µ</span>
    <br>
    <span>Function Calling</span>
  </a>
  <a class="next" href="https://Aliga123.github.io/posts/ai-llm/langsmith/">
    <span class="title">ä¸‹ä¸€é¡µ Â»</span>
    <br>
    <span>LangSmith</span>
  </a>
</nav>

        </footer>
    </div>

<style>
    .comments_details summary::marker {
        font-size: 20px;
        content: 'ğŸ‘‰å±•å¼€è¯„è®º';
        color: var(--content);
    }
    .comments_details[open] summary::marker{
        font-size: 20px;
        content: 'ğŸ‘‡å…³é—­è¯„è®º';
        color: var(--content);
    }
</style>


<div>
    <details class="comments_details">
        <summary style="cursor: pointer; margin: 50px 0 20px 0;width: 130px;">
            <span style="font-size: 20px;color: var(--content);">...</span>
        </summary>
        <div id="tcomment"></div>
    </details>
    <script src="https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js">
    </script>
    <script>
        twikoo.init({
            envId:  null ,
        el: "#tcomment",
            lang: 'zh-CN',
            region:  null ,
        path: window.TWIKOO_MAGIC_PATH||window.location.pathname,
        })
    </script>
</div>
</article>
</main>

<footer class="footer">
    <span>
        Copyright
        &copy;
        2020-2024
        <a href="https://Aliga123.github.io/" style="color:#939393;">aliga&#39;s Blog</a>
        All Rights Reserved
    </span>
    <a href="https://beian.miit.gov.cn/" target="_blank" style="color:#939393;">å¡«å†™è‡ªå·±çš„å¤‡æ¡ˆå·</a>&nbsp;
    <span>
        <a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=null"
           style="display:inline-block;text-decoration:none;height:20px;color:#939393;">
            <img src="%e5%a1%ab%e8%87%aa%e5%b7%b1%e7%9a%84%e5%85%ac%e5%ae%89%e5%9b%be%e6%a0%87%e9%93%be%e6%8e%a5" style="float:left;margin: 0px 5px 0px 0px;"/>
            å¡«è‡ªå·±çš„å…¬ç½‘å®‰å¤‡
        </a>
    </span>
    <span id="busuanzi_container">
        <span class="fa fa-user"></span> <span id="busuanzi_value_site_uv"></span>
        <span class="fa fa-eye"></span> <span id="busuanzi_value_site_pv"></span>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>

<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 50) {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"aliga's Blog"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"aliga's Blog"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'å¤åˆ¶';

        function copyingDone() {
            copybutton.innerText = 'å·²å¤åˆ¶ï¼';
            setTimeout(() => {
                copybutton.innerText = 'å¤åˆ¶';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                let text = codeblock.textContent +
                    '\r\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"aliga's Blog"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                navigator.clipboard.writeText(text);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {}
            selection.removeRange(range);
        });

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>
</body>

</html>
